{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEWS Python Notebook 07c: generating L8 WQ data\n",
    "\n",
    "**Author**: Eric Lehmann, CSIRO Data61  \n",
    "**Date**: 25 May 2016 (slight update: July 15, 2016)\n",
    "\n",
    "**Note**: The Python code below is \"rudimentary\" etc. etc. Priority is here given to code interpretability rather than execution efficiency.\n",
    "\n",
    "**Note**: this notebook should be accessible and viewable at [https://github.com/eric542/agdc_v2/tree/master/notebooks](https://github.com/eric542/agdc_v2/tree/master/notebooks).\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we introduce / review the following concepts:\n",
    "\n",
    "* loading PQ-masked Landsat / WQ data for an area and time range of interest, using AE/EE\n",
    "* \"removing\" duplicated dates from the dataset\n",
    "* reproject WQ data to geodetic\n",
    "* loading WOFS data for the same area\n",
    "* creating and applying the WOFS/water masks to the WQ data\n",
    "* doing the above for multiple dates and saving each result to .png file\n",
    "* saving associated ancillary data in .json format\n",
    "\n",
    "This version `07c` is an updated duplicate of `07b` applied to L8 dataset.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This notebook summarises the processes developed / tested in the previous notebooks in this series. The aim is to demonstrate the automated production of WQ maps for a given lake / waterbody of interest, ready to be ingested in the AEWS visualisation interface module.\n",
    "\n",
    "The code below generates output files for *all* dates available on the NCI for the region of interest, or *all* dates in the selected date range. Ultimately, this process will only be executed once, with follow-up executions only updating the \"database\" with the latest imagery, if new data is available.\n",
    "\n",
    "The processes that are still missing from a \"full-blown\" AEWS implementation are:\n",
    "\n",
    "* automatically reading the lat/lon extents of the waterbodies of interest from .shp file\n",
    "* checking whether new data is available and needs to be processed (rather than reprocessing the entire time series)\n",
    "* application of SWIR \"glint\" filter to the data\n",
    "* comparison of SWIR filter against WOFS for further \"low water\" flagging\n",
    "\n",
    "Also, further possible improvements are:\n",
    "\n",
    "* use NDexpression language to optimise some steps, e.g. water masking of time series in one operation\n",
    "* use updated AE/EE, e.g. to reproject data with `execute_plan()`\n",
    "\n",
    "This (Jupyter) notebook was written for use on the NCI's VDI system, with the following pre-loaded module:\n",
    "\n",
    "```\n",
    " $ module use /g/data/v10/public/modules/modulefiles --append\n",
    " $ module load agdc-py2-prod \n",
    "```\n",
    "\n",
    "**NOTE**: the specific module loaded here (`agdc-py2-prod`) is different from the module loaded in earlier notebooks (`agdc-py2-dev`)! While the earlier module contained only Landsat 5 data, the `agdc-py2-prod` module links to a (different) AGDC database containing the following NBART/NBAR/PQA datasets:\n",
    "\n",
    "* Landsat 8: 2013\n",
    "* Landsat 7: 2013\n",
    "* Landsat 5: 2006/2013\n",
    "\n",
    "It is unclear whether the API functions in these 2 modules are identical or represent different versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr, ogr\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "from __future__ import print_function\n",
    "\n",
    "import datacube.api   # import the AGDC v2 API, with pre-loaded 'agds-py2-dev' (or similar) NCI module\n",
    "\n",
    "from datetime import datetime\n",
    "from datacube.analytics.analytics_engine import AnalyticsEngine\n",
    "from datacube.execution.execution_engine import ExecutionEngine\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def expand_mask(mask_arr, npix):\n",
    "    # Uses the True/False (masked/non-masked) values in the array 'mask_arr' and \n",
    "    # expands the True values spatially by 'npix' pixels. The value 'npix' can be\n",
    "    # non-integer, i.e. the mask can be expanded by any spatial distance.\n",
    "    nmid = np.floor(npix)\n",
    "    nmax = int( nmid*2 + 1 )\n",
    "    struc = np.zeros((nmax, nmax), dtype='bool')\n",
    "    for ii in range(nmax):   # create desired binary structure for morphological operation\n",
    "        for jj in range(ii,nmax):\n",
    "            if pdist( [[nmid,nmid], [ii,jj]] ) <= npix:\n",
    "                struc[ii,jj] = True\n",
    "                struc[jj,ii] = True\n",
    "    return ndimage.binary_dilation(mask_arr, structure=struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Investigation of available L8 dataset:\n",
    "\n",
    "# dc = datacube.api.API()\n",
    "# print( dc.list_field_values('product') )  # [u'pqa', u'ortho', u'DEM', u'satellite_telemetry_data', u'nbart', u'nbar']\n",
    "# print( dc.list_field_values('platform') ) # [u'LANDSAT_8', u'LANDSAT_5', u'SRTM', u'LANDSAT_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query = {   # whole of Australia!!\n",
    "#     'product': 'nbar',\n",
    "#     'platform': 'LANDSAT_8',\n",
    "# }\n",
    "# desc = dc.get_descriptor(query, include_storage_units=False)\n",
    "# pprint(desc)\n",
    "\n",
    "#{u'ls8_nbar_albers': {'dimensions': [u'time', u'y', u'x'],\n",
    "#                      'irregular_indices': {u'time': array(['2013-03-19T11:30:08.884702000+1100',\n",
    "#       '2013-03-19T11:30:32.710102000+1100',\n",
    "#       '2013-03-19T11:30:56.540705000+1100', ...,\n",
    "#       '2013-12-31T12:29:23.398040000+1100',\n",
    "#       '2013-12-31T12:29:47.347583000+1100',\n",
    "#       '2013-12-31T12:30:11.303643000+1100'], dtype='datetime64[ns]')},\n",
    "#                      'result_max': (numpy.datetime64('2013-12-31T12:30:11.303643000+1100'),\n",
    "#                                     -900012.5,\n",
    "#                                     2399987.5),\n",
    "#                      'result_min': (numpy.datetime64('2013-03-19T11:30:08.884702000+1100'),\n",
    "#                                     -4999987.5,\n",
    "#                                     -1999987.5),\n",
    "#                      'result_shape': (6030, 164000, 176000),\n",
    "#                      'variables': {u'band_1': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999},\n",
    "#                                    u'band_2': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999},\n",
    "#                                    u'band_3': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999},\n",
    "#                                    u'band_4': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999},\n",
    "#                                    u'band_5': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999},\n",
    "#                                    u'band_6': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999},\n",
    "#                                    u'band_7': {'datatype_name': dtype('int16'),\n",
    "#                                                'nodata_value': -999}}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# query = { 'product':   'pqa',\n",
    "#           'platform':  'LANDSAT_8',\n",
    "#           'dimensions': { 'x':    {'range': (149.06, 149.17)},\n",
    "#                           'y':    {'range': (-35.33, -35.27)},\n",
    "#                           'time': {'range': ((2013, 1, 1), (2014, 1, 1))} } }\n",
    "# desc = dc.get_descriptor(query, include_storage_units=False)\n",
    "# pprint(desc)\n",
    "\n",
    "#{u'ls8_pq_albers': {'dimensions': [u'time', u'y', u'x'],\n",
    "#                    'irregular_indices': {u'time': array(['2013-04-12T09:46:35.385577000+1000',\n",
    "#       '2013-04-19T09:52:06.192160000+1000',\n",
    "#        [...]\n",
    "#       '2013-12-31T10:51:42.297674000+1100',\n",
    "#       '2013-12-31T10:52:06.251744000+1100'], dtype='datetime64[ns]')},\n",
    "#                    'result_max': (numpy.datetime64('2013-12-31T10:52:06.251744000+1100'),\n",
    "#                                   -3955437.5,\n",
    "#                                   1554312.5),\n",
    "#                    'result_min': (numpy.datetime64('2013-04-12T09:46:35.385577000+1000'),\n",
    "#                                   -3963362.5,\n",
    "#                                   1543537.5),\n",
    "#                    'result_shape': (68, 318, 432),\n",
    "#                    'variables': {u'pixelquality': {'datatype_name': dtype('int16'),\n",
    "#                                                    'nodata_value': None}}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "\n",
    "Here are the input parameters needed from the user:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimensions = { 'longitude': {'range': (149.06, 149.17)},   # selected region\n",
    "               'latitude':  {'range': (-35.33, -35.27)},\n",
    "               'time':      {'range': ((2013, 1, 1), (2014, 1, 1))} }\n",
    "\n",
    "lake_str = \"Lake_Burley_Griffin\"        # (string) name of selected location / region\n",
    "lake_dispname = \"Lake Burley Griffin\"   # (display) name of selected location / region\n",
    "\n",
    "water_mask_thr = 90.0     # water percentage level to use to create water mask from WOFS\n",
    "water_mask_buffer = 1.5   # buffer distance in pixels to expand the land mask by, i.e. \"shrihnk\" water mask\n",
    "min_valid_pix_thr = 10.0  # minimum percentage of valid water pixels necessary to save image to file\n",
    "\n",
    "save_basedir = '/g/data/jr4/vis_data_v1.1/'   # where the data is saved\n",
    "\n",
    "WQ_type = \"WQ_(B2+B3)/2\"  # Type of WQ info generated by this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading / creating PQ-masked WQ data\n",
    "\n",
    "**Important**: due to the availability of the Coastal Aerosol band in LS8 (not available in LS5,7), the \"usual\" Landsat bands are offset by one. For instance, the \"usual\" (green) band nr. 2 in LS5,7 actually corresponds to 'band_3' in LS8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datacube.analytics.analytics_engine:Initialise Analytics Module.\n",
      "INFO:datacube.execution.execution_engine:Initialise Execution Module.\n"
     ]
    }
   ],
   "source": [
    "ae = AnalyticsEngine()\n",
    "ee = ExecutionEngine()\n",
    "\n",
    "aeB2 = ae.create_array(('LANDSAT_8', 'nbar'), ['band_3'], dimensions, 'aeB2')\n",
    "aeB3 = ae.create_array(('LANDSAT_8', 'nbar'), ['band_4'], dimensions, 'aeB3')\n",
    "aePQ = ae.create_array(('LANDSAT_8', 'pqa'), ['pixelquality'], dimensions, 'aePQ')\n",
    "aeWQ = ae.apply_expression([aeB2, aeB3], '((array1 + array2) * 0.5)', 'aeWQ')\n",
    "aeWQ_PQmsk = ae.apply_expression([aeWQ, aePQ], 'array1{array2}', 'aeWQ_PQmsk')\n",
    "ee.execute_plan(ae.plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting Y/M/D dates:\n",
    "LS_dates = ee.cache['aeWQ_PQmsk']['array_indices']['time']\n",
    "n_dates = len( LS_dates )\n",
    "strLS_dates = np.zeros(n_dates).astype('str')\n",
    "for ii in range(n_dates):\n",
    "    strLS_dates[ii] = str( LS_dates[ii] )[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting PQ-masked WQ arrays:\n",
    "WQ_TS_xarr = ee.cache['aeWQ_PQmsk']['array_result']['aeWQ_PQmsk']\n",
    "WQ_TS_arr = WQ_TS_xarr.values   # Time series dataset of WQ arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Removing\" duplicated dates:\n",
    "rem_ind = np.zeros(n_dates).astype('bool')   # keep track of which duplicated dates to remove\n",
    "\n",
    "for ind in range(1,n_dates):\n",
    "    dup_ind = np.where( strLS_dates[:ind]==strLS_dates[ind] )[0]   # check for duplicated date up to current index\n",
    "    if len( dup_ind )!=0:   # found (at least) one duplicate\n",
    "        dup_ind = dup_ind[0]   # only use the first index if multiple dates returned\n",
    "        rem_ind[ind] = True    # remove current date index\n",
    "        \n",
    "        ind_n_nans = np.sum( np.isnan( WQ_TS_arr[ind] ) )   # nr of NaN pixels in each image\n",
    "        dup_n_nans = np.sum( np.isnan( WQ_TS_arr[dup_ind] ) )\n",
    "        \n",
    "        if ind_n_nans==0:   # current data has no NaN's, use it instead of duplicate date (copy it to lowest index)\n",
    "            WQ_TS_arr[dup_ind] = WQ_TS_arr[ind]\n",
    "        elif dup_n_nans!=0:   # if duplicate date has no NaN's: do nothing (use it instead of current date)\n",
    "            if dup_n_nans<ind_n_nans:   # duplicate date has less NaN's: fill it in with current data\n",
    "                tmp = np.where( np.isnan(WQ_TS_arr[dup_ind]) )\n",
    "                WQ_TS_arr[dup_ind][tmp] = WQ_TS_arr[ind][tmp]\n",
    "            else:   # dup_n_nans>=ind_n_nans -- duplicate date has more NaN's: use it to fill in current data\n",
    "                tmp = np.where( np.isnan(WQ_TS_arr[ind]) )\n",
    "                WQ_TS_arr[ind][tmp] = WQ_TS_arr[dup_ind][tmp]\n",
    "                WQ_TS_arr[dup_ind] = WQ_TS_arr[ind]   # save results to lowest date index\n",
    "\n",
    "WQ_TS_arr = WQ_TS_arr[ ~rem_ind ]\n",
    "strLS_dates = strLS_dates[ ~rem_ind ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading WOFS data and creating water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will create issues if regions spans multiple tiles...\n",
    "mean_lon = np.mean( dimensions['longitude']['range'] )\n",
    "mean_lat = np.mean( dimensions['latitude']['range'] )\n",
    "base_folder = '/g/data2/fk4/wofs/current/pyramids/WaterSummary/0/'   # where geoTiff WOFS data is located\n",
    "WOFS_fname = base_folder + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( mean_lon, math.floor(mean_lat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "WOFS_array = WOFS_dataset.ReadAsArray()\n",
    "WOFS_geotx = list( WOFS_dataset.GetGeoTransform() )\n",
    "WOFS_proj = WOFS_dataset.GetProjection()   # geodetic lat/lon\n",
    "WOFS_srs = osr.SpatialReference( wkt=WOFS_proj )\n",
    "\n",
    "WOFS_lonvec = np.arange(WOFS_array.shape[0]) * WOFS_geotx[1] + WOFS_geotx[0]\n",
    "WOFS_latvec = np.arange(WOFS_array.shape[1]) * WOFS_geotx[5] + WOFS_geotx[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract WOFS over region of interest:\n",
    "yind_min = np.where( WOFS_lonvec>=dimensions['longitude']['range'][0] )[0][0]\n",
    "yind_max = np.where( WOFS_lonvec<=dimensions['longitude']['range'][1] )[0][-1]\n",
    "WOFS_lonvec = WOFS_lonvec[yind_min:yind_max+1]\n",
    "\n",
    "xind_min = np.where( WOFS_latvec>=dimensions['latitude']['range'][1] )[0][-1]\n",
    "xind_max= np.where( WOFS_latvec<=dimensions['latitude']['range'][0] )[0][0]\n",
    "WOFS_latvec = WOFS_latvec[xind_min:xind_max+1]\n",
    "\n",
    "WOFS_array = WOFS_array[xind_min:xind_max+1, yind_min:yind_max+1]\n",
    "WOFS_geotx[0] = WOFS_geotx[0] + yind_min*WOFS_geotx[1]\n",
    "WOFS_geotx[3] = WOFS_geotx[3] + xind_min*WOFS_geotx[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generating the water / land mask:\n",
    "land_mask = ( WOFS_array<=water_mask_thr )\n",
    "land_mask = expand_mask( land_mask, water_mask_buffer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojecting WQ data, water masking, and saving to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get info about the Landsat data...\n",
    "\n",
    "# The following should be read from the data directly but this info does not seem \n",
    "# to be available/extractable from AE/EE's outputs... (?)\n",
    "dc = datacube.api.API()\n",
    "query = { 'product': 'nbar',\n",
    "          'platform': 'LANDSAT_8',\n",
    "          'dimensions': dimensions }\n",
    "LSdata = dc.get_data( query )\n",
    "\n",
    "LS_proj_str = LSdata['coordinate_reference_systems'][1]['reference_system_definition']\n",
    "LS_proj = osr.SpatialReference( wkt=LS_proj_str )\n",
    "\n",
    "# Things that we can infer from the AE/EE results:\n",
    "LSxvec = ee.cache['aeWQ_PQmsk']['array_result']['aeWQ_PQmsk']['x'].values\n",
    "LSyvec = ee.cache['aeWQ_PQmsk']['array_result']['aeWQ_PQmsk']['y'].values\n",
    "LSpix_size = np.mean(LSxvec[1:] - LSxvec[:-1])\n",
    "LS_geotx = ( LSxvec[0], LSpix_size, 0, LSyvec[0], 0, -LSpix_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WQ gdal dataset\n",
    "WQ_dx, WQ_dy = WQ_TS_arr[0].shape\n",
    "gdal_WQ_data = gdal.GetDriverByName( 'MEM' ).Create('', WQ_dy, WQ_dx, 1, gdal.GDT_Float32)\n",
    "gdal_WQ_data.SetGeoTransform( LS_geotx )\n",
    "gdal_WQ_data.SetProjection( LS_proj.ExportToWkt() );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reprojection variables\n",
    "(WOFS_dx, WOFS_dy) = WOFS_array.shape\n",
    "gdalgeo_WQ_data = gdal.GetDriverByName( 'MEM' ).Create('', WOFS_dy, WOFS_dx, 1, gdal.GDT_Float32)\n",
    "gdalgeo_WQ_data.SetGeoTransform( WOFS_geotx )\n",
    "gdalgeo_WQ_data.SetProjection( WOFS_proj );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise contents.json data\n",
    "contents_json_list = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create directory if necessary:\n",
    "save_dir = save_basedir + lake_str + \"/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping over all dates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_datetime = datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "for dateind in range( len(strLS_dates) ):\n",
    "    # \"Save\" WQ data as gdal dataset:\n",
    "    gdal_WQ_data.GetRasterBand(1).WriteArray( WQ_TS_arr[dateind], 0, 0);\n",
    "    \n",
    "    # Reprojecting the WQ gdal dataset to match the WOFS coordinates:\n",
    "    gdalgeo_WQ_data.GetRasterBand(1).WriteArray( np.ones((WOFS_dx,WOFS_dy))*np.nan, 0, 0)\n",
    "    res = gdal.ReprojectImage( gdal_WQ_data, gdalgeo_WQ_data, \n",
    "                               LS_proj.ExportToWkt(), WOFS_proj, \n",
    "                               gdal.GRA_Bilinear )  # gdal.GRA_NearestNeighbour by default\n",
    "    \n",
    "    WQ_array = gdalgeo_WQ_data.ReadAsArray()   # reprojected result\n",
    "    WQ_array[land_mask] = np.nan   # masking non-water pixels out\n",
    "    \n",
    "    # Saving image to file, only if enough valid data...\n",
    "    n_water_pix = np.sum( ~land_mask )\n",
    "    n_valid_pix = np.sum( ~np.isnan(WQ_array) )\n",
    "    if (100.0*n_valid_pix/n_water_pix)>=min_valid_pix_thr:\n",
    "        f_name_noext = \"WQ_\" + lake_str + \"_\" + strLS_dates[dateind]\n",
    "        f_basename = save_dir + f_name_noext\n",
    "        \n",
    "        png_fname = f_basename + \".png\"\n",
    "        img.imsave(png_fname, WQ_array)\n",
    "        \n",
    "        # Save ancillary data to .json file:\n",
    "        json_dict = { 'name': lake_str,\n",
    "                      'displayName': lake_dispname,\n",
    "                      'date': strLS_dates[dateind],\n",
    "                      'image': \"./\" + f_name_noext + \".png\",\n",
    "                      'DateCreated': cur_datetime,\n",
    "                      'EPSG': WOFS_srs.GetAttrValue(\"AUTHORITY\", 1),\n",
    "                      'TLcorner': [WOFS_geotx[0], WOFS_geotx[3]],\n",
    "                      'LRcorner': [WOFS_geotx[0]+WOFS_dy*WOFS_geotx[1], WOFS_geotx[3]+WOFS_dx*WOFS_geotx[5]],\n",
    "                      'minValue': float( np.nanmin( WQ_array ) ),\n",
    "                      'maxValue': float( np.nanmax( WQ_array ) ),\n",
    "                      'meanValue': float( np.nanmean( WQ_array ) ),\n",
    "                      'medianValue': float( np.nanmedian( WQ_array ) ),\n",
    "                      'flag': 'undefined',       # this needs to be determined from the data...\n",
    "                      'lakeType': 'undefined' }  # this should be user-defined somehow...\n",
    "        \n",
    "        json_fname = f_basename + \".json\"\n",
    "        with open(json_fname, \"w\") as outfile:\n",
    "            json.dump(json_dict, outfile, indent=4)\n",
    "        \n",
    "        short_json_dict = { 'name': lake_str,\n",
    "                            'displayName': lake_dispname,\n",
    "                            'date': strLS_dates[dateind],\n",
    "                            'type': WQ_type,\n",
    "                            'image': \"./\" + f_name_noext + \".png\",\n",
    "                            'json': \"./\" + f_name_noext + \".json\",\n",
    "                            'DateCreated': cur_datetime,\n",
    "                            'flag': 'undefined',\n",
    "                            'lakeType': 'undefined' }\n",
    "        contents_json_list.append( short_json_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the `contents.json` file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_basename = save_dir + \"contents.json\"\n",
    "with open(f_basename, \"w\") as outfile:\n",
    "    json.dump(contents_json_list, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
