{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEWS Python Notebook 07b: saving WQ data to file\n",
    "\n",
    "**Author**: Eric Lehmann, CSIRO Data61  \n",
    "**Date**: 03 May 2016 (slight update: July 15, 2016)\n",
    "\n",
    "**Note**: The Python code below is \"rudimentary\" etc. etc. Priority is here given to code interpretability rather than execution efficiency.\n",
    "\n",
    "**Note**: this notebook should be accessible and viewable at [https://github.com/eric542/agdc_v2/tree/master/notebooks](https://github.com/eric542/agdc_v2/tree/master/notebooks).\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this notebook, we introduce / review the following concepts:\n",
    "\n",
    "* loading PQ-masked Landsat / WQ data for an area and time range of interest, using AE/EE\n",
    "* \"removing\" duplicated dates from the dataset\n",
    "* reproject WQ data to geodetic\n",
    "* loading WOFS data for the same area\n",
    "* creating and applying the WOFS/water masks to the WQ data\n",
    "* doing the above for multiple dates and saving each result to .png file\n",
    "* saving associated ancillary data in .json format\n",
    "\n",
    "This version `07b` automates the above steps to generate `.png` / `.json` files for an entire time series.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This notebook summarises the processes developed / tested in the previous notebooks in this series. The aim is to demonstrate the automated production of WQ maps for a given lake / waterbody of interest, ready to be ingested in the AEWS visualisation interface module.\n",
    "\n",
    "The code below generates output files for *all* dates available on the NCI for the region of interest, or *all* dates in the selected date range. Ultimately, this process will only be executed once, with follow-up executions only updating the \"database\" with the latest imagery, if new data is available.\n",
    "\n",
    "The processes that are still missing from a \"full-blown\" AEWS implementation are:\n",
    "\n",
    "* automatically reading the lat/lon extents of the waterbodies of interest from .shp file\n",
    "* checking whether new data is available and needs to be processed (rather than reprocessing the entire time series)\n",
    "* application of SWIR \"glint\" filter to the data\n",
    "* comparison of SWIR filter against WOFS for further \"low water\" flagging\n",
    "\n",
    "Also, further possible improvements are:\n",
    "\n",
    "* use NDexpression language to optimise some steps, e.g. water masking of time series in one operation\n",
    "* use updated AE/EE, e.g. to reproject data with `execute_plan()`\n",
    "\n",
    "This (Jupyter) notebook was written for use on the NCI's VDI system, with the following pre-loaded module:\n",
    "\n",
    "```\n",
    " $ module use /g/data/v10/public/modules/modulefiles --append\n",
    " $ module load agdc-py2-dev\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from osgeo import gdal, osr, ogr\n",
    "import math\n",
    "import json\n",
    "\n",
    "from pprint import pprint\n",
    "from __future__ import print_function\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = (10,10)   # increase plot size a bit...\n",
    "rcParams['axes.formatter.useoffset'] = False   # disable scalar formatter / offset in axes labels\n",
    "\n",
    "import datacube.api   # import the AGDC v2 API, with pre-loaded 'agds-py2-dev' (or similar) NCI module\n",
    "\n",
    "from datetime import datetime\n",
    "from datacube.analytics.analytics_engine import AnalyticsEngine\n",
    "from datacube.execution.execution_engine import ExecutionEngine\n",
    "from datacube.analytics.utils.analytics_utils import plot\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def expand_mask(mask_arr, npix):\n",
    "    # Uses the True/False (masked/non-masked) values in the array 'mask_arr' and \n",
    "    # expands the True values spatially by 'npix' pixels. The value 'npix' can be\n",
    "    # non-integer, i.e. the mask can be expanded by any spatial distance.\n",
    "    nmid = np.floor(npix)\n",
    "    nmax = int( nmid*2 + 1 )\n",
    "    struc = np.zeros((nmax, nmax), dtype='bool')\n",
    "    for ii in range(nmax):   # create desired binary structure for morphological operation\n",
    "        for jj in range(ii,nmax):\n",
    "            if pdist( [[nmid,nmid], [ii,jj]] ) <= npix:\n",
    "                struc[ii,jj] = True\n",
    "                struc[jj,ii] = True\n",
    "    return ndimage.binary_dilation(mask_arr, structure=struc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs\n",
    "\n",
    "Here are the input parameters needed from the user:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimensions = { 'longitude': {'range': (149.06, 149.17)},   # selected region\n",
    "               'latitude':  {'range': (-35.33, -35.27)},\n",
    "               'time':      { 'range': ((1992, 1, 1), (1997, 1, 1))} }\n",
    "# Note: the NCI database currently has a bug in the 1991 data, with apparently one less file of PQ data\n",
    "#       than the nr of files of Landsat data. Selecting the entire LBG database thus crashes the code\n",
    "#       below. Starting from 1992 solves the issue.\n",
    "\n",
    "lake_str = \"Lake_Burley_Griffin\"        # (string) name of selected location / region\n",
    "lake_dispname = \"Lake Burley Griffin\"   # (display) name of selected location / region\n",
    "\n",
    "water_mask_thr = 90.0     # water percentage level to use to create water mask from WOFS\n",
    "water_mask_buffer = 1.5   # buffer distance in pixels to expand the land mask by, i.e. \"shrihnk\" water mask\n",
    "min_valid_pix_thr = 10.0  # minimum percentage of valid water pixels necessary to save image to file\n",
    "\n",
    "save_basedir = '/g/data/jr4/vis_data_v1.0/'   # where the data is saved\n",
    "\n",
    "WQ_type = \"WQ_(B2+B3)/2\"  # Type of WQ info generated by this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading / creating PQ-masked WQ data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datacube.analytics.analytics_engine:Initialise Analytics Module.\n",
      "INFO:datacube.execution.execution_engine:Initialise Execution Module.\n"
     ]
    }
   ],
   "source": [
    "ae = AnalyticsEngine()\n",
    "ee = ExecutionEngine()\n",
    "\n",
    "aeB2 = ae.create_array(('LANDSAT_5', 'NBAR'), ['band_20'], dimensions, 'aeB2')\n",
    "aeB3 = ae.create_array(('LANDSAT_5', 'NBAR'), ['band_30'], dimensions, 'aeB3')\n",
    "aePQ = ae.create_array(('LANDSAT_5', 'PQ'), ['band_pixelquality'], dimensions, 'aePQ')\n",
    "aeWQ = ae.apply_expression([aeB2, aeB3], '((array1 + array2) * 0.5)', 'aeWQ')\n",
    "aeWQ_PQmsk = ae.apply_expression([aeWQ, aePQ], 'array1{array2}', 'aeWQ_PQmsk')\n",
    "ee.execute_plan(ae.plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting Y/M/D dates:\n",
    "L5_dates = ee.cache['aeWQ_PQmsk']['array_indices']['time']\n",
    "n_dates = len( L5_dates )\n",
    "strL5_dates = np.zeros(n_dates).astype('str')\n",
    "for ii in range(n_dates):\n",
    "    strL5_dates[ii] = str( L5_dates[ii] )[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting PQ-masked WQ arrays:\n",
    "WQ_TS_xarr = ee.cache['aeWQ_PQmsk']['array_result']['aeWQ_PQmsk']\n",
    "WQ_TS_arr = WQ_TS_xarr.values   # Time series dataset of WQ arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"Removing\" duplicated dates:\n",
    "rem_ind = np.zeros(n_dates).astype('bool')   # keep track of which duplicated dates to remove\n",
    "\n",
    "for ind in range(1,n_dates):\n",
    "    dup_ind = np.where( strL5_dates[:ind]==strL5_dates[ind] )[0]   # check for duplicated date up to current index\n",
    "    if len( dup_ind )!=0:   # found (at least) one duplicate\n",
    "        dup_ind = dup_ind[0]   # only use the first index if multiple dates returned\n",
    "        rem_ind[ind] = True    # remove current date index\n",
    "        \n",
    "        ind_n_nans = np.sum( np.isnan( WQ_TS_arr[ind] ) )   # nr of NaN pixels in each image\n",
    "        dup_n_nans = np.sum( np.isnan( WQ_TS_arr[dup_ind] ) )\n",
    "        \n",
    "        if ind_n_nans==0:   # current data has no NaN's, use it instead of duplicate date (copy it to lowest index)\n",
    "            WQ_TS_arr[dup_ind] = WQ_TS_arr[ind]\n",
    "        elif dup_n_nans!=0:   # if duplicate date has no NaN's: do nothing (use it instead of current date)\n",
    "            if dup_n_nans<ind_n_nans:   # duplicate date has less NaN's: fill it in with current data\n",
    "                tmp = np.where( np.isnan(WQ_TS_arr[dup_ind]) )\n",
    "                WQ_TS_arr[dup_ind][tmp] = WQ_TS_arr[ind][tmp]\n",
    "            else:   # dup_n_nans>=ind_n_nans -- duplicate date has more NaN's: use it to fill in current data\n",
    "                tmp = np.where( np.isnan(WQ_TS_arr[ind]) )\n",
    "                WQ_TS_arr[ind][tmp] = WQ_TS_arr[dup_ind][tmp]\n",
    "                WQ_TS_arr[dup_ind] = WQ_TS_arr[ind]   # save results to lowest date index\n",
    "\n",
    "WQ_TS_arr = WQ_TS_arr[ ~rem_ind ]\n",
    "strL5_dates = strL5_dates[ ~rem_ind ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading WOFS data and creating water mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will create issues if regions spans multiple tiles...\n",
    "mean_lon = np.mean( dimensions['longitude']['range'] )\n",
    "mean_lat = np.mean( dimensions['latitude']['range'] )\n",
    "base_folder = '/g/data2/fk4/wofs/current/pyramids/WaterSummary/0/'   # where geoTiff WOFS data is located\n",
    "WOFS_fname = base_folder + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( mean_lon, math.floor(mean_lat) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "WOFS_array = WOFS_dataset.ReadAsArray()\n",
    "WOFS_geotx = list( WOFS_dataset.GetGeoTransform() )\n",
    "WOFS_proj = WOFS_dataset.GetProjection()   # geodetic lat/lon\n",
    "WOFS_srs = osr.SpatialReference( wkt=WOFS_proj )\n",
    "\n",
    "WOFS_lonvec = np.arange(WOFS_array.shape[0]) * WOFS_geotx[1] + WOFS_geotx[0]\n",
    "WOFS_latvec = np.arange(WOFS_array.shape[1]) * WOFS_geotx[5] + WOFS_geotx[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract WOFS over region of interest:\n",
    "yind_min = np.where( WOFS_lonvec>=dimensions['longitude']['range'][0] )[0][0]\n",
    "yind_max = np.where( WOFS_lonvec<=dimensions['longitude']['range'][1] )[0][-1]\n",
    "WOFS_lonvec = WOFS_lonvec[yind_min:yind_max+1]\n",
    "\n",
    "xind_min = np.where( WOFS_latvec>=dimensions['latitude']['range'][1] )[0][-1]\n",
    "xind_max= np.where( WOFS_latvec<=dimensions['latitude']['range'][0] )[0][0]\n",
    "WOFS_latvec = WOFS_latvec[xind_min:xind_max+1]\n",
    "\n",
    "WOFS_array = WOFS_array[xind_min:xind_max+1, yind_min:yind_max+1]\n",
    "WOFS_geotx[0] = WOFS_geotx[0] + yind_min*WOFS_geotx[1]\n",
    "WOFS_geotx[3] = WOFS_geotx[3] + xind_min*WOFS_geotx[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generating the water / land mask:\n",
    "land_mask = ( WOFS_array<=water_mask_thr )\n",
    "land_mask = expand_mask( land_mask, water_mask_buffer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reprojecting WQ data, water masking, and saving to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get info about the Landsat data...\n",
    "\n",
    "# The following should be read from the data directly but this info does not seem \n",
    "# to be available/extractable from AE/EE's outputs... (?)\n",
    "dc = datacube.api.API()\n",
    "query = { 'product': 'NBAR',\n",
    "          'platform': 'LANDSAT_5',\n",
    "          'dimensions': dimensions }\n",
    "L5data = dc.get_data( query )\n",
    "\n",
    "L5_proj_str = L5data['coordinate_reference_systems'][1]['reference_system_definition']\n",
    "L5_proj = osr.SpatialReference( wkt=L5_proj_str )\n",
    "\n",
    "# Things that we can infer from the AE/EE results:\n",
    "L5xvec = ee.cache['aeWQ_PQmsk']['array_result']['aeWQ_PQmsk']['x'].values\n",
    "L5yvec = ee.cache['aeWQ_PQmsk']['array_result']['aeWQ_PQmsk']['y'].values\n",
    "L5pix_size = np.mean(L5xvec[1:] - L5xvec[:-1])\n",
    "L5_geotx = ( L5xvec[0], L5pix_size, 0, L5yvec[0], 0, -L5pix_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# WQ gdal dataset\n",
    "WQ_dx, WQ_dy = WQ_TS_arr[0].shape\n",
    "gdal_WQ_data = gdal.GetDriverByName( 'MEM' ).Create('', WQ_dy, WQ_dx, 1, gdal.GDT_Float32)\n",
    "gdal_WQ_data.SetGeoTransform( L5_geotx )\n",
    "gdal_WQ_data.SetProjection( L5_proj.ExportToWkt() );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reprojection variables\n",
    "(WOFS_dx, WOFS_dy) = WOFS_array.shape\n",
    "gdalgeo_WQ_data = gdal.GetDriverByName( 'MEM' ).Create('', WOFS_dy, WOFS_dx, 1, gdal.GDT_Float32)\n",
    "gdalgeo_WQ_data.SetGeoTransform( WOFS_geotx )\n",
    "gdalgeo_WQ_data.SetProjection( WOFS_proj );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialise contents.json data\n",
    "contents_json_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping over all dates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dateind in range( len(strL5_dates) ):\n",
    "    # \"Save\" WQ data as gdal dataset:\n",
    "    gdal_WQ_data.GetRasterBand(1).WriteArray( WQ_TS_arr[dateind], 0, 0);\n",
    "    \n",
    "    # Reprojecting the WQ gdal dataset to match the WOFS coordinates:\n",
    "    gdalgeo_WQ_data.GetRasterBand(1).WriteArray( np.ones((WOFS_dx,WOFS_dy))*np.nan, 0, 0)\n",
    "    res = gdal.ReprojectImage( gdal_WQ_data, gdalgeo_WQ_data, \n",
    "                               L5_proj.ExportToWkt(), WOFS_proj, \n",
    "                               gdal.GRA_Bilinear )  # gdal.GRA_NearestNeighbour by default\n",
    "    \n",
    "    WQ_array = gdalgeo_WQ_data.ReadAsArray()   # reprojected result\n",
    "    WQ_array[land_mask] = np.nan   # masking non-water pixels out\n",
    "    \n",
    "    # Saving image to file, only if enough valid data...\n",
    "    n_water_pix = np.sum( ~land_mask )\n",
    "    n_valid_pix = np.sum( ~np.isnan(WQ_array) )\n",
    "    if (100.0*n_valid_pix/n_water_pix)>=min_valid_pix_thr:\n",
    "        f_basename = save_basedir + lake_str + \"/WQ_\" + lake_str + \"_\" + strL5_dates[dateind]\n",
    "        \n",
    "        png_fname = f_basename + \".png\"\n",
    "        img.imsave(png_fname, WQ_array)\n",
    "        \n",
    "        # Save ancillary data to .json file:\n",
    "        json_dict = { 'name': lake_str,\n",
    "                      'displayName': lake_dispname,\n",
    "                      'date': strL5_dates[dateind],\n",
    "                      'image': png_fname,\n",
    "                      'EPSG': WOFS_srs.GetAttrValue(\"AUTHORITY\", 1),\n",
    "                      'TLcorner': [WOFS_geotx[0], WOFS_geotx[3]],\n",
    "                      'LRcorner': [WOFS_geotx[0]+WOFS_dy*WOFS_geotx[1], WOFS_geotx[3]+WOFS_dx*WOFS_geotx[5]],\n",
    "                      'minValue': float( np.nanmin( WQ_array ) ),\n",
    "                      'maxValue': float( np.nanmax( WQ_array ) ),\n",
    "                      'meanValue': float( np.nanmean( WQ_array ) ),\n",
    "                      'medianValue': float( np.nanmedian( WQ_array ) ),\n",
    "                      'flag': 'undefined',       # this needs to be determined from the data...\n",
    "                      'lakeType': 'undefined' }  # this should be user-defined somehow...\n",
    "        \n",
    "        json_fname = f_basename + \".json\"\n",
    "        with open(json_fname, \"w\") as outfile:\n",
    "            json.dump(json_dict, outfile, indent=4)\n",
    "        \n",
    "        short_json_dict = { 'name': lake_str,\n",
    "                            'displayName': lake_dispname,\n",
    "                            'date': strL5_dates[dateind],\n",
    "                            'type': WQ_type,\n",
    "                            'image': png_fname,\n",
    "                            'json': json_fname,\n",
    "                            'flag': 'undefined',\n",
    "                            'lakeType': 'undefined' }\n",
    "        contents_json_list.append( short_json_dict )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the `contents.json` file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_basename = save_basedir + lake_str + \"/contents.json\"\n",
    "with open(f_basename, \"w\") as outfile:\n",
    "    json.dump(contents_json_list, outfile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
