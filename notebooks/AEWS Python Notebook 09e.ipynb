{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEWS Python Notebook 09e: complete (pre-batch) WQ algorithm\n",
    "\n",
    "**Author**: Eric Lehmann, CSIRO Data61  \n",
    "**Date**: Aug. 26, 2016.\n",
    "\n",
    "**Note**: this notebook should be accessible and viewable at https://github.com/eric542/agdc_v2/tree/master/notebooks.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook brings together all the AEWS components tested / implemented in earlier notebooks. It implements the whole AEWS workflow, starting from the selected ROI and AGDC data, and ultimately creating a NetCDF dataset containing a time series of WQ maps with associated ancillary information. For testing purposes, this is here carried out for a single selected polygon, and thus represents a pre-batch version. A fully automated batch script executed on the NCI will automatically iterate through all the polygons of interest.\n",
    "\n",
    "This notebook version (09e) started off as a copy of '_AEWS Python Notebook 09d_', updated to parallelise the main waterbody-processing loop.\n",
    "\n",
    "Main changes from previous version include:\n",
    "\n",
    "+ re-definition of lakes processing loop as function\n",
    "+ changes to the handling of API connections / calls to avoid recurring connections to the database\n",
    "+ attempt to reduce potential race conditions from multiple threads when reading from `.shp` file\n",
    "\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This (Jupyter) notebook was written for use on the NCI's VDI system, with the following pre-loaded module:\n",
    "\n",
    "```\n",
    " $ module use /g/data/v10/public/modules/modulefiles --append\n",
    " $ module load agdc-py2-prod \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ogr, osr, gdal\n",
    "import xarray as xr\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "from netCDF4 import Dataset, num2date, date2num\n",
    "from datetime import timedelta, datetime    # date\n",
    "\n",
    "from matplotlib.path import Path   # for point-in-polygon\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import time as timer\n",
    "from pprint import pprint\n",
    "from __future__ import print_function\n",
    "\n",
    "import datacube.api\n",
    "from datacube.analytics.analytics_engine import AnalyticsEngine\n",
    "from datacube.execution.execution_engine import ExecutionEngine\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams as rcp\n",
    "\n",
    "from sklearn import linear_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_mask(mask_arr, npix):\n",
    "    # Uses the True/False (masked/non-masked) values in the array 'mask_arr' and \n",
    "    # expands the True values spatially by 'npix' pixels. The value 'npix' can be\n",
    "    # non-integer, i.e. the mask can be expanded by any spatial distance.\n",
    "    nmid = np.floor(npix)\n",
    "    nmax = int( nmid*2 + 1 )\n",
    "    struc = np.zeros((nmax, nmax), dtype='bool')\n",
    "    for ii in range(nmax):   # create desired binary structure for morphological operation\n",
    "        for jj in range(ii,nmax):\n",
    "            if pdist( [[nmid,nmid], [ii,jj]] ) <= npix:\n",
    "                struc[ii,jj] = True\n",
    "                struc[jj,ii] = True\n",
    "    return ndimage.binary_dilation(mask_arr, structure=struc)\n",
    "\n",
    "\n",
    "def merge_dup_dates(xrda):\n",
    "    # Takes in an xarray.DataArray 'xrda' as input and merges \n",
    "    # datasets (time slices) within it that have the same dates.\n",
    "    # Returns the modified DataArray and vector of selected dates.\n",
    "    dates = xrda.coords['time'].values\n",
    "    n_dates = len( dates )\n",
    "    \n",
    "    # Convert UTM times to local dates (d/m/y only):\n",
    "    str_dates = np.zeros(n_dates).astype('str')\n",
    "    for ii in range(n_dates):\n",
    "        str_dates[ii] = str( dates[ii] )[:10]\n",
    "    \n",
    "    # Remove duplicated dates:\n",
    "    rem_ind = np.zeros(n_dates).astype('bool')   # keep track of which duplicated dates to remove\n",
    "    for ind in range(1,n_dates):\n",
    "        dup_ind = np.where( str_dates[:ind]==str_dates[ind] )[0]   # check for duplicated date up to current index\n",
    "        if len( dup_ind )!=0:   # found (at least) one duplicate\n",
    "            dup_ind = dup_ind[0]   # only use the first index if multiple dates returned\n",
    "            rem_ind[ind] = True    # remove current date index\n",
    "            ind_n_nans = np.sum( np.isnan( xrda[ind] ) ) # nr of NaN pixels in each image\n",
    "            dup_n_nans = np.sum( np.isnan( xrda[dup_ind] ) )\n",
    "            if ind_n_nans==0:     # current data has no NaN's, use it instead of duplicate date (copy it to lowest index)\n",
    "                xrda[dup_ind] = xrda[ind]\n",
    "                xrda['time'].values[dup_ind] = xrda['time'].values[ind]\n",
    "            elif dup_n_nans!=0:   # if duplicate date has no NaN's: do nothing (use it instead of current date)\n",
    "                if dup_n_nans<ind_n_nans:   # duplicate date has less NaN's: fill it in with current data\n",
    "                    tmp = np.where( np.isnan(xrda[dup_ind]) )\n",
    "                    xrda[dup_ind].values[tmp] = xrda[ind].values[tmp]\n",
    "                else:   # dup_n_nans>=ind_n_nans -- duplicate date has more NaN's: use it to fill in current data\n",
    "                    tmp = np.where( np.isnan(xrda[ind]) )\n",
    "                    xrda[ind].values[tmp] = xrda[dup_ind].values[tmp]\n",
    "                    xrda[dup_ind] = xrda[ind]   # save results to lowest date index, in case >2 slices have same date\n",
    "                    xrda['time'].values[dup_ind] = xrda['time'].values[ind]\n",
    "    \n",
    "    return xrda[~rem_ind]\n",
    "\n",
    "\n",
    "def load_wofs_data(wofs_path, min_lon, max_lon, min_lat, max_lat):\n",
    "    # Returns array of WOFS data for given extents. Can read data from \n",
    "    # multiple adjacent tiles if necessary, at most 2 in either direction.\n",
    "    lon_idx1 = np.floor(min_lon); lon_idx2 = np.floor(max_lon)\n",
    "    dlon = abs(lon_idx1-lon_idx2)\n",
    "    lat_idx1 = np.floor(min_lat); lat_idx2 = np.floor(max_lat)\n",
    "    dlat = abs(lat_idx1-lat_idx2)\n",
    "    if ( dlon>1 or dlat>1 ):\n",
    "        raise RuntimeError(\"Lake extents span 3 or more WOFS tiles (in at least one dimension).\")\n",
    "    adj_tile_lon = (dlon==1)    # two adjacent 'horizontal' WOFS tiles\n",
    "    adj_tile_lat = (dlat==1)    # two adjacent 'vertical' WOFS tiles\n",
    "\n",
    "    # Load the first (top-left) of (potentially) several WOFS tiles:\n",
    "    WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(min_lon), np.floor(max_lat) )\n",
    "    WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "    if WOFS_dataset==None:\n",
    "        raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "    WOFS_array = WOFS_dataset.ReadAsArray()\n",
    "    WOFS_geotx = list( WOFS_dataset.GetGeoTransform() )\n",
    "    WOFS_proj = WOFS_dataset.GetProjection()   # geodetic lat/lon\n",
    "    # WOFS_srs = osr.SpatialReference( wkt=WOFS_proj )\n",
    "    WOFS_lonvec = np.arange(WOFS_array.shape[0]) * WOFS_geotx[1] + WOFS_geotx[0]\n",
    "    WOFS_latvec = np.arange(WOFS_array.shape[1]) * WOFS_geotx[5] + WOFS_geotx[3]\n",
    "    WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "    if ( adj_tile_lon ):   # if 2 horizontal tiles, load (top) right tile\n",
    "        WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(max_lon), np.floor(max_lat) )\n",
    "        WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "        if WOFS_dataset==None:\n",
    "            raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "        wa2 = WOFS_dataset.ReadAsArray()\n",
    "        wg2 = list( WOFS_dataset.GetGeoTransform() )\n",
    "        WOFS_lonvec = np.concatenate(( WOFS_lonvec, np.arange(wa2.shape[0]) * wg2[1] + wg2[0] ),0)\n",
    "        WOFS_array = np.concatenate((WOFS_array,wa2),1)   # column bind\n",
    "        WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "    if ( adj_tile_lat ):   # if 2 vertical tiles: load bottom (left) tile\n",
    "        WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(min_lon), np.floor(min_lat) )\n",
    "        WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "        if WOFS_dataset==None:\n",
    "            raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "        wa2 = WOFS_dataset.ReadAsArray()\n",
    "        wg2 = list( WOFS_dataset.GetGeoTransform() )\n",
    "        WOFS_latvec = np.concatenate(( WOFS_latvec, np.arange(wa2.shape[1]) * wg2[5] + wg2[3] ),0)\n",
    "        WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "        if ( adj_tile_lon ):   # if 2 horizontal & 2 vertical tiles: load bottom right tile\n",
    "            WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(max_lon), np.floor(min_lat) )\n",
    "            WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "            if WOFS_dataset==None:\n",
    "                raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "            wa3 = WOFS_dataset.ReadAsArray()\n",
    "            wa2 = np.concatenate((wa2,wa3),1)   # column bind\n",
    "            WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "        WOFS_array = np.concatenate((WOFS_array,wa2),0)   # row bind\n",
    "    \n",
    "    # Extract WOFS over region of interest:\n",
    "    yind_min = np.where( WOFS_lonvec>=min_lon )[0][0]\n",
    "    yind_max = np.where( WOFS_lonvec<=max_lon )[0][-1]\n",
    "    WOFS_lonvec = WOFS_lonvec[yind_min:yind_max+1]\n",
    "    xind_max = np.where( WOFS_latvec>=min_lat )[0][-1]\n",
    "    xind_min = np.where( WOFS_latvec<=max_lat )[0][0]\n",
    "    WOFS_latvec = WOFS_latvec[xind_min:xind_max+1]\n",
    "    WOFS_array = WOFS_array[xind_min:xind_max+1, yind_min:yind_max+1]\n",
    "    WOFS_geotx[0] = WOFS_geotx[0] + yind_min*WOFS_geotx[1]\n",
    "    WOFS_geotx[3] = WOFS_geotx[3] + xind_min*WOFS_geotx[5]\n",
    "\n",
    "    return WOFS_array, WOFS_proj, WOFS_geotx, WOFS_lonvec, WOFS_latvec\n",
    "\n",
    "\n",
    "def solar_day(utc, lon):\n",
    "    # returns approx. solar data from 'utc' time and average 'lon'.\n",
    "    # Based on GA's implementation, available at:\n",
    "    # https://github.com/data-cube/agdc-v2/blob/38a3430b3f49977f3b07355ba0d5d2d4ad7bf965/datacube/api/geo_xarray.py\n",
    "    seconds_per_degree = 240\n",
    "    offset_seconds = int(lon * seconds_per_degree)\n",
    "    offset = np.timedelta64(offset_seconds, 's')\n",
    "    return utc + offset\n",
    "\n",
    "\n",
    "def write_to_log(logf,strn,print_screen=True):\n",
    "    # Prints info 'strn' to command line as well as log file 'logf'\n",
    "    logf.write( strn + '\\n')\n",
    "    if print_screen and not no_screen_outputs:\n",
    "        print( strn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ftr_loop(ftr):\n",
    "    # Function implementing the processing of waterbody 'ftr'.\n",
    "    # 'Returns' a list of lists: [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "    \n",
    "    # loop 'outputs':\n",
    "    out_log_str = ''\n",
    "    out_nc_flag = False\n",
    "    out_cpu_arr = np.ones((1,6))*np.nan\n",
    "\n",
    "    poly_t0 = timer.time()\n",
    "    \n",
    "    # Log info:\n",
    "    out_log_str += \"Processing polygon nr. {} of {} ...\".format(ftr+1,n_lakes) + '\\n'\n",
    "    out_log_str += \"  Polygon name: {}\".format(lakes_dispname_list[ftr]) + '\\n'\n",
    "    out_log_str += \"  Surface area is {} ha.\".format(round(shape_list[ftr]/10000.0,2)) + '\\n'\n",
    "\n",
    "    \n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: ROI polygon coordinates and extents\n",
    "    ###########################################################################\n",
    "\n",
    "    # Read in polygon coordinates: reading from .shp file from multiple parallel threads \n",
    "    # appears to lead to race conditions at times; the code below tries to mitigate this.\n",
    "    np.random.seed(ftr)\n",
    "    \n",
    "    lake_ftr = lakes_lyr.GetFeature( lakes_ftr_idx[ftr] )\n",
    "    \n",
    "    tmp = lake_ftr.GetGeometryRef()\n",
    "    cnt = 0\n",
    "    while tmp==None:\n",
    "        if cnt==0:\n",
    "            out_log_str += \"  Problem accessing lat/lon coords for this polygon (lake_ftr.GetGeometryRef is None).\" + '\\n'\n",
    "            return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "        cnt -= 1\n",
    "        timer.sleep(np.random.random())\n",
    "        tmp = lake_ftr.GetGeometryRef()\n",
    "    \n",
    "    ring = tmp.GetGeometryRef(0)\n",
    "    cnt = 0\n",
    "    while ring==None:\n",
    "        if cnt==0:\n",
    "            out_log_str += \"  Problem accessing lat/lon coords for this polygon (ring is None).\" + '\\n'\n",
    "            return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "        cnt -= 1\n",
    "        timer.sleep(np.random.random())\n",
    "        ring = tmp.GetGeometryRef(0)\n",
    "    \n",
    "    tmp = ring.GetPoints()\n",
    "    cnt = 0\n",
    "    while tmp==None:\n",
    "        if cnt==0:\n",
    "            out_log_str += \"  Problem accessing lat/lon coords for this polygon (ring.GetPoints is None).\" + '\\n'\n",
    "            return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "        cnt -= 1\n",
    "        timer.sleep(np.random.random())\n",
    "        tmp = ring.GetPoints()\n",
    "    \n",
    "    tmp = np.array(tmp)\n",
    "    if tmp.shape[0]==0 or tmp.shape[1]<2:\n",
    "        out_log_str += \"  Problem accessing lat/lon coords for this polygon (no coords).\" + '\\n'\n",
    "        return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "    \n",
    "    poly_array_geo = tmp[:,0:2]    # lon / lat coords of polygon\n",
    "\n",
    "    tmp1 = min(poly_array_geo[:,0]) - lake_buffer_width_geo\n",
    "    min_lon = (tmp1 // pix_size_geo) * pix_size_geo\n",
    "\n",
    "    tmp2 = max(poly_array_geo[:,0]) + lake_buffer_width_geo\n",
    "    max_lon = (tmp2 // pix_size_geo) * pix_size_geo + pix_size_geo\n",
    "\n",
    "    tmp3 = min(poly_array_geo[:,1]) - lake_buffer_width_geo\n",
    "    min_lat = (tmp3 // pix_size_geo) * pix_size_geo\n",
    "\n",
    "    tmp4 = max(poly_array_geo[:,1]) + lake_buffer_width_geo\n",
    "    max_lat = (tmp4 // pix_size_geo) * pix_size_geo + pix_size_geo\n",
    "\n",
    "    # Coordinates of resulting WQ time series:\n",
    "    lon_vec = np.arange(min_lon, max_lon+pix_size_geo, pix_size_geo)\n",
    "    lat_vec = np.arange(min_lat, max_lat+pix_size_geo, pix_size_geo)\n",
    "    mean_lon = (max_lon+min_lon)/2.0\n",
    "\n",
    "    # Log info:\n",
    "    out_log_str += \"  Polygon extents: min lon.: {} --> nearest pixel: {}\".format(tmp1,min_lon) + '\\n'\n",
    "    out_log_str += \"                   max lon.: {} --> nearest pixel: {}\".format(tmp2,max_lon) + '\\n'\n",
    "    out_log_str += \"                   min lat.: {} --> nearest pixel: {}\".format(tmp3,min_lat) + '\\n'\n",
    "    out_log_str += \"                   min lat.: {} --> nearest pixel: {}\".format(tmp3,min_lat) + '\\n'\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    ### Iteration over polygons: check for existing dataset, determine ROI date range\n",
    "    ######################################################################################\n",
    "\n",
    "    roi_file_str = lakes_name_list[ftr] + \".nc\"\n",
    "    roi_save_name = save_dir_path + roi_file_str\n",
    "    roi_file_exists = os.path.isfile( roi_save_name )\n",
    "    \n",
    "    nodata_log_name = nodata_dir_path + lakes_name_list[ftr] + \".log\"\n",
    "    nodata_file_exists = os.path.isfile( nodata_log_name )\n",
    "\n",
    "    roi_start_date = datetime( int(start_date[:4]), int(start_date[5:7]), int(start_date[8:10]) )\n",
    "    \n",
    "    roi_append_nc = False\n",
    "    if append_to_existing_nc:\n",
    "        if roi_file_exists:\n",
    "            roi_append_nc = True\n",
    "            \n",
    "            # read file attributes:\n",
    "            roi_nc_grp = Dataset(roi_save_name, mode='r')\n",
    "            roi_nc_dates = roi_nc_grp.variables['time']\n",
    "\n",
    "            # consistency checks between datasets:\n",
    "            if not (roi_nc_grp.variables['lat'][:]==lat_vec[::-1]).all():\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different latitudes.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not (roi_nc_grp.variables['lon'][:]==lon_vec).all():\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different longitudes.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not getattr(roi_nc_grp,'ResultsVersion')==results_version:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different WQ outputs version.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not getattr(roi_nc_grp,'WQtype')==WQ_type:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different WQ type.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not roi_nc_dates.calendar==netcdf_time_calendar:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different time calendar.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not roi_nc_dates.units==netcdf_time_units:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different time units.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not getattr(roi_nc_grp,'Name')==lakes_name_list[ftr]:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different lake name.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not getattr(roi_nc_grp,'DisplayName')==lakes_dispname_list[ftr]:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different display name.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not getattr(roi_nc_grp,'EPSG')==geo_proj.GetAttrValue(\"AUTHORITY\", 1):\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different EPSG.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            if not getattr(roi_nc_grp,'LakeType')==\"undefined\":   # clear/deep,turbid/shallow ... should be defined somehow!\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different lake type.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "\n",
    "            tmp = getattr(roi_nc_grp,'GAR_thresholds')\n",
    "            c1 = np.size(tmp)!=np.size(alert_thresholds)\n",
    "            c2 = (np.size(tmp)==1 and (~np.isnan(tmp) | ~np.isnan(alert_thresholds)))\n",
    "            c3 = (np.size(tmp)==2 & ~(tmp==alert_thresholds).all())\n",
    "            if c1 or c2 or c3:\n",
    "                out_log_str += \"  Error appending to existing NetCDF '{}': different GAR thresholds.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "\n",
    "            if len( roi_nc_dates )==0:\n",
    "                out_log_str += \"  Error: existing dataset '{}' has 0 dates.\".format(roi_save_name) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "\n",
    "            # get start date from existing .nc file:\n",
    "            tmp = getattr(roi_nc_grp,'LastProcDate')   # avoids re-processing no-data time window / dates\n",
    "            roi_nc_last_date = datetime(int(tmp[:4]), int(tmp[5:7]), int(tmp[8:10]) )\n",
    "            roi_start_date = roi_nc_last_date + timedelta(days=1)\n",
    "\n",
    "            roi_nc_grp.close()\n",
    "            out_log_str += \"  Found existing NetCDF file '{}'; will append to it.\".format(roi_save_name) + '\\n'\n",
    "            out_log_str += \"    Last processed date of existing dataset: \" + roi_nc_last_date.strftime('%Y-%m-%d') + '\\n'\n",
    "        \n",
    "        else:   # no existing file: cannot append\n",
    "            tmp = \"  Could not find existing NetCDF file '{}' to append to; will create new file.\".format(roi_save_name)\n",
    "            out_log_str += tmp + '\\n'\n",
    "        #---end: if roi_file_exists\n",
    "            \n",
    "        # check for existing 'no-data' log file: if found, load LastProcDate\n",
    "        if nodata_file_exists:   # polygon has been processed previously, check LastProcDate\n",
    "            out_log_str += \"  Found existing \\\"no-data\\\" file '{}'.\".format(nodata_log_name) + '\\n'\n",
    "\n",
    "            # read contents:\n",
    "            dic = {}\n",
    "            with open(nodata_log_name,\"r\") as ndf:\n",
    "                for line in ndf:\n",
    "                   (key, val) = line[:-1].split(None,1)\n",
    "                   dic[key] = val\n",
    "\n",
    "            tmp = (not dic.has_key('Name') or dic['Name']!=lakes_name_list[ftr])\n",
    "            tmp = tmp or (not dic.has_key('DisplayName') or dic['DisplayName']!=lakes_dispname_list[ftr])\n",
    "            tmp = tmp or (not dic.has_key('ResultsVersion') or dic['ResultsVersion']!=results_version)\n",
    "            tmp = tmp or not dic.has_key('LastProcDate')\n",
    "            if tmp:\n",
    "                out_log_str += \"    Inconsistent \\\"no-data\\\" file; will be ignored.\" + '\\n'\n",
    "            else:\n",
    "                tmp = dic['LastProcDate']   # avoids re-processing no-data time window / dates\n",
    "                nodata_last_date = datetime(int(tmp[:4]), int(tmp[5:7]), int(tmp[8:10]) )\n",
    "                nodata_start_date = nodata_last_date + timedelta(days=1)\n",
    "                out_log_str += \"    Last previously processed date: \" + nodata_last_date.strftime('%Y-%m-%d') + '\\n'\n",
    "                \n",
    "                if nodata_start_date>roi_start_date or not roi_append_nc:\n",
    "                    roi_start_date = nodata_start_date\n",
    "            #---end: if tmp\n",
    "        #---end: if nodata_file_exists\n",
    "    #---end: if append_to_existing_nc\n",
    "\n",
    "    if end_date==None:\n",
    "        tmp = datetime.today()\n",
    "        roi_end_date = datetime(tmp.year,tmp.month,tmp.day)\n",
    "    else:\n",
    "        roi_end_date = datetime( int(end_date[:4]), int(end_date[5:7]), int(end_date[8:10]) )\n",
    "    roi_end_date = roi_end_date + timedelta(days=1)\n",
    "\n",
    "    tmp = \"  Date range for processing: \" + roi_start_date.strftime('%Y-%m-%d') + \" (inclusive) to \" \n",
    "    tmp = tmp + roi_end_date.strftime('%Y-%m-%d') + \" (exclusive)\"\n",
    "    out_log_str += tmp + '\\n'\n",
    "\n",
    "    if roi_start_date >= roi_end_date:\n",
    "        out_log_str += \"  No data to process: start date of time window is on or after end date.\" + '\\n'\n",
    "        return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: load and process AGDC time series\n",
    "    ###########################################################################\n",
    "    \n",
    "    # AGDC defs:\n",
    "    dimensions = { 'lon' : { 'range': (min_lon, max_lon) },\n",
    "                   'lat' : { 'range': (max_lat, min_lat) },\n",
    "                   'time': { 'range': ( (roi_start_date.year, roi_start_date.month, roi_start_date.day), \n",
    "                                        (roi_end_date.year, roi_end_date.month, roi_end_date.day) ) } }\n",
    "\n",
    "    # Cycling through satellites:\n",
    "    n_sats = len(WQ_sats)\n",
    "    proc_init = True\n",
    "    wofs_init = True\n",
    "    wofs_error = False\n",
    "    concat_init = True\n",
    "    n_dates_orig = 0\n",
    "    last_proc_date = datetime(1,1,1)\n",
    "    \n",
    "    dc_api = datacube.api.API()\n",
    "    ae = AnalyticsEngine(api=dc_api)\n",
    "    ee = ExecutionEngine(api=dc_api)\n",
    "\n",
    "    for sat in range(n_sats):\n",
    "\n",
    "        ### Check for satellite availability:\n",
    "        sat_defs = WQ_sats.values()[sat]\n",
    "        sat_str = sat_defs['API_sat_str']\n",
    "        plat_str = sat_defs['API_plat_str']\n",
    "        out_log_str += \"  Processing {} data (satellite {} of {}) ... \".format(plat_str,sat+1,n_sats) + '\\n'\n",
    "\n",
    "        if sat_str not in list(dc_prods['name']):\n",
    "            out_log_str += \"    Satellite data '{}' not available in current database.\".format(sat_str) + '\\n'\n",
    "            continue   # next satellite\n",
    "        \n",
    "\n",
    "        ### Check for data availability:\n",
    "        query = { 'platform': plat_str,\n",
    "                  'product': nbar_prod,\n",
    "                  'dimensions': dimensions }\n",
    "        try:\n",
    "            tmp = dc_api.get_descriptor(query, include_storage_units=False)\n",
    "            if len( tmp[sat_str]['irregular_indices']['time'] )==0:\n",
    "                out_log_str += \"    No '{}' satellite data available in current database, for the selected time window and ROI.\".format(sat_str) + '\\n'\n",
    "                continue\n",
    "        except:\n",
    "            out_log_str += \"    No '{}' satellite data available in current database, for the selected time window and ROI.\".format(sat_str) + '\\n'\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        ### Load data using AE/EE:\n",
    "        ae.plan = list()   # appears to be \"resetting\" the AE definitions...\n",
    "        # ae = AnalyticsEngine()\n",
    "        # ee = ExecutionEngine()\n",
    "        \n",
    "        ae_green = ae.create_array( (plat_str, nbar_prod), [gbnd], dimensions, 'ae_green' )\n",
    "        ae_red   = ae.create_array( (plat_str, nbar_prod), [rbnd], dimensions, 'ae_red' )\n",
    "        ae_swir2 = ae.create_array( (plat_str, nbar_prod), [s2bnd], dimensions, 'ae_swir2' )\n",
    "        aePQ     = ae.create_array( (plat_str, pq_prod), [pqbnd], dimensions, 'aePQ' )\n",
    "\n",
    "        # selected WQ algorithm:\n",
    "        if WQ_type=='Alg1:(red+green)/2':\n",
    "            aeWQ = ae.apply_expression( [ae_green, ae_red], '((array1 + array2) * 0.5)', 'aeWQ' )\n",
    "        elif WQ_type=='Alg2:TwoBand_TSS_calib':\n",
    "            cfac = sat_defs['Alg2_calib_fac']\n",
    "            cexp = sat_defs['Alg2_calib_exp']\n",
    "            # tmp = '( {} * ((array1 + array2)*0.00005)**{} )'.format(cfac,cexp)   # logical formulation, doesn't work!\n",
    "            tmp = '{}*exp(log((array1+array2)*0.00005)*{})'.format(cfac,cexp)   # hack to implement the power function...\n",
    "            aeWQ = ae.apply_expression( [ae_green, ae_red], tmp, 'aeWQ' )\n",
    "        else:\n",
    "            raise NotImplementedError( \"Water quality algorithm '{}' not implemented.\".format(WQ_type) )\n",
    "\n",
    "        ee.execute_plan(ae.plan)\n",
    "        WQ_xarray = ee.cache['aeWQ']['array_result']['aeWQ']\n",
    "        PQ_xarray = ee.cache['aePQ']['array_result'][pqbnd]\n",
    "        S2_xarray = ee.cache['ae_swir2']['array_result'][s2bnd]\n",
    "        \n",
    "        WQ_xarray = WQ_xarray.load()   # allows about 8-fold speed-up by avoiding multiple lazy (re-)loading...\n",
    "        PQ_xarray = PQ_xarray.load()\n",
    "        S2_xarray = S2_xarray.load()\n",
    "\n",
    "        # convert UTC dates to solar days:\n",
    "        for ii in range(WQ_xarray.shape[0]):\n",
    "            WQ_xarray['time'].values[ii] = solar_day(WQ_xarray['time'].values[ii], mean_lon).astype('datetime64[ns]')\n",
    "        for ii in range(PQ_xarray.shape[0]):\n",
    "            PQ_xarray['time'].values[ii] = solar_day(PQ_xarray['time'].values[ii], mean_lon).astype('datetime64[ns]')\n",
    "        for ii in range(S2_xarray.shape[0]):\n",
    "            S2_xarray['time'].values[ii] = solar_day(S2_xarray['time'].values[ii], mean_lon).astype('datetime64[ns]')\n",
    "        \n",
    "        n_dates = WQ_xarray.shape[0]\n",
    "        n_dates_orig = n_dates_orig + n_dates\n",
    "        tmp1 = WQ_xarray['time'].values\n",
    "        tmp = \"    {} time slices were found for the selected time window and ROI\".format(n_dates)\n",
    "        tmp = tmp + \" ({} to {}).\".format( str(tmp1[0])[:10], str(tmp1[-1])[:10] )\n",
    "        out_log_str += tmp + '\\n'\n",
    "        \n",
    "        tmp = datetime( int(str(tmp1[-1])[:4]), int(str(tmp1[-1])[5:7]), int(str(tmp1[-1])[8:10]) )\n",
    "        if last_proc_date<tmp:\n",
    "            last_proc_date = tmp\n",
    "        \n",
    "        \n",
    "        ### Initialise proj. and poly. variables: once-only operations\n",
    "        if proc_init:\n",
    "            proc_init = False\n",
    "\n",
    "            # Satellite projection data: assume all satellites have same projection!\n",
    "            tmp = dc_prods[dc_prods.name==sat_str]['crs'].item().wkt\n",
    "            sat_proj = osr.SpatialReference( wkt=tmp )\n",
    "\n",
    "            sat_xvec = WQ_xarray['x'].values\n",
    "            sat_yvec = WQ_xarray['y'].values\n",
    "            npix_x = len(sat_xvec)\n",
    "            npix_y = len(sat_yvec)\n",
    "\n",
    "            sat_pix_size = sat_xvec[1] - sat_xvec[0]   # assume uniform square pixels\n",
    "            sat_geotx = ( sat_xvec[0], sat_pix_size, 0, sat_yvec[0], 0, -sat_pix_size )\n",
    "\n",
    "            # Polygon mask:\n",
    "            geo_to_sat_proj_tx = osr.CoordinateTransformation( geo_proj, sat_proj )\n",
    "            tmp = geo_to_sat_proj_tx.TransformPoints( poly_array_geo )\n",
    "            poly_array_xy = np.array( tmp )[:,:2]\n",
    "            polyPath = Path(poly_array_xy)\n",
    "            poly_mask_xy = np.ones(WQ_xarray.shape[1:], dtype='bool')\n",
    "\n",
    "            jj1 = np.where( sat_xvec<min(poly_array_xy[:,0]) )[0][-1]\n",
    "            jj2 = np.where( sat_xvec>max(poly_array_xy[:,0]) )[0][0]\n",
    "            ii1 = np.where( sat_yvec>max(poly_array_xy[:,1]) )[0][-1]\n",
    "            ii2 = np.where( sat_yvec<min(poly_array_xy[:,1]) )[0][0]\n",
    "            njj = jj2 - jj1 + 1\n",
    "\n",
    "            for ii in range( ii1, ii2+1 ):\n",
    "                tmp = np.stack( (sat_xvec[jj1:jj2+1], sat_yvec[ii]*np.ones(njj)), axis=1 )\n",
    "                poly_mask_xy[ii,jj1:jj2+1] = ~polyPath.contains_points( tmp )\n",
    "\n",
    "            n_pix_poly = np.sum( ~poly_mask_xy )\n",
    "            n_valid_pix_min = lake_min_valid_prc * n_pix_poly / 100.0\n",
    "\n",
    "            # Buffered polygon mask:\n",
    "            poly_mask_xy_bufd = poly_mask_xy.copy()\n",
    "            poly_mask_xy_bufd = expand_mask( poly_mask_xy_bufd, polygon_buffer_width )\n",
    "        #---end: if proc_init\n",
    "\n",
    "\n",
    "        ### Process time series: apply non-buffered-polygon + PQ masking to WQ and SWIR2 data\n",
    "        keep_ind = np.zeros(n_dates).astype('bool')    # keep track of which dates (if any) to remove\n",
    "        PQ_dates = PQ_xarray['time'].values\n",
    "\n",
    "        for dd in range(n_dates):\n",
    "            cur_date = WQ_xarray['time'].values[dd]    # current date\n",
    "            idx = np.where(PQ_dates==cur_date)[0]\n",
    "            if len(idx)==0:    \n",
    "                continue   # time slice not found in PQ dataset: remove WQ slice (unable to mask properly)\n",
    "\n",
    "            # time slice found in PQ dataset: mask and keep date in time series\n",
    "            keep_ind[dd] = True\n",
    "            tmp = PQ_xarray[idx[0]].values.astype(int)\n",
    "            msk = ( (tmp & cloud_bit_mask)!=cloud_bit_mask ).astype(bool)\n",
    "            msk = expand_mask( msk, cloud_buffer_width )    # expanded cloud mask\n",
    "            msk = msk | ( (tmp & other_bit_mask)!=other_bit_mask ).astype(bool)   # saturation, sea, contiguity\n",
    "            msk = msk | poly_mask_xy   # non-buffered-polygon mask\n",
    "            WQ_xarray.values[dd][msk] = np.nan\n",
    "            S2_xarray.values[dd][msk] = np.nan\n",
    "        #---end: for dd in range(n_dates)\n",
    "\n",
    "        tmp = np.sum(keep_ind)\n",
    "        if n_dates!=tmp:\n",
    "            out_log_str += \"    {} time slices were removed due to discrepancies with the PQ dataset.\".format( n_dates-tmp) + '\\n'\n",
    "        if tmp==0:\n",
    "            out_log_str += \"    --> No data left for this satellite.\" + '\\n'\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[keep_ind]   # remove unwanted time slices\n",
    "        S2_xarray = S2_xarray[keep_ind]\n",
    "\n",
    "\n",
    "        ### Merge dates: this should theoretically process WQ and SWIR2 data identically...\n",
    "        WQ_xarray = merge_dup_dates( WQ_xarray.load() )\n",
    "        S2_xarray = merge_dup_dates( S2_xarray.load() )\n",
    "        out_log_str += \"    {} time slices left after merging duplicated dates.\".format(WQ_xarray.shape[0]) + '\\n'\n",
    "\n",
    "\n",
    "        ### Remove \"empty\" time slices (ie. nr of valid pixels below threshold):\n",
    "        tmp = ( ~np.isnan(WQ_xarray) ).sum('x').sum('y').values\n",
    "        idx = np.where( tmp>=n_valid_pix_min )[0]\n",
    "        n_dates = len(idx)\n",
    "        out_log_str += \"    {} time slices left following PQ & polygon masking.\".format(n_dates) + '\\n'\n",
    "        if n_dates==0:\n",
    "            out_log_str += \"    --> No data left for this satellite.\" + '\\n'\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[idx]\n",
    "        S2_xarray = S2_xarray[idx]\n",
    "\n",
    "\n",
    "        ### Initialise WOFS variables: once-only (only if some data available / left)\n",
    "        if wofs_init:\n",
    "            wofs_init = False\n",
    "\n",
    "            # load WOFS data and reproject to UTM / satellite projection:\n",
    "            try:\n",
    "                wofs_array, wofs_proj, wofs_geotx, tmp1, tmp2 = load_wofs_data(\n",
    "                                                wofs_dir_path, min_lon, max_lon, min_lat, max_lat )\n",
    "            except RuntimeError as err:\n",
    "                out_log_str += \"  Error loading WOFS data: {}\".format(err) + '\\n'\n",
    "                wofs_error = True\n",
    "                break\n",
    "\n",
    "            # WOFS gdal dataset:\n",
    "            nr, nc = wofs_array.shape\n",
    "            gdal_data = gdal.GetDriverByName( 'MEM' ).Create('', nc, nr, 1, gdal.GDT_Float32)\n",
    "            gdal_data.SetGeoTransform( wofs_geotx )\n",
    "            gdal_data.SetProjection( wofs_proj )\n",
    "            gdal_data.GetRasterBand(1).WriteArray( wofs_array, 0, 0)\n",
    "\n",
    "            # reprojected WOFS gdal dataset:\n",
    "            gdal_proj_data = gdal.GetDriverByName( 'MEM' ).Create('', len(sat_xvec), len(sat_yvec), 1, gdal.GDT_Float32)\n",
    "            gdal_proj_data.SetGeoTransform( sat_geotx )\n",
    "            gdal_proj_data.SetProjection( sat_proj.ExportToWkt() )\n",
    "            gdal_proj_data.GetRasterBand(1).WriteArray( np.zeros((len(sat_yvec),len(sat_xvec))), 0, 0) # prefill with 0\n",
    "            tmp = gdal.ReprojectImage( gdal_data, gdal_proj_data,\n",
    "                                       wofs_proj, sat_proj.ExportToWkt(),\n",
    "                                       gdal.GRA_Bilinear )    # gdal.GRA_NearestNeighbour by default\n",
    "\n",
    "            # WOFS mask, also masked by polygon (both non-buffered):\n",
    "            wofs_mask_xy = gdal_proj_data.ReadAsArray()\n",
    "            wofs_mask_xy = (wofs_mask_xy<wofs_prc_thr)\n",
    "            wofs_mask_xy = wofs_mask_xy | poly_mask_xy\n",
    "            n_pix_wofs = (~wofs_mask_xy).sum()\n",
    "        #---end: if wofs_init\n",
    "\n",
    "\n",
    "        ### Process time series: calculate WOFS and poly stats / flags, apply SWIR2 filter\n",
    "        low_water_flag_list = [\"undefined\" for ii in range(n_dates)]\n",
    "        wofs_flag_list = [\"ephemeral\" for ii in range(n_dates)]\n",
    "\n",
    "        for dd in range(n_dates):\n",
    "            WQ_arr = WQ_xarray[dd].values   # poly & PQ masked\n",
    "\n",
    "            # polygon stats:\n",
    "            tmp = np.sum( ~np.isnan(WQ_arr) )\n",
    "            n_pix_poly_nan = n_pix_poly - tmp   # low-water stats\n",
    "\n",
    "            # WOFS masking & stats (for stats only, not applied to WQ):\n",
    "            if n_pix_wofs!=0:\n",
    "                wq_tmp = WQ_arr.copy()\n",
    "                wq_tmp[wofs_mask_xy] = np.nan\n",
    "                tmp = np.sum( ~np.isnan(wq_tmp) )\n",
    "                n_pix_wofs_nan = n_pix_wofs - tmp   # WOFS stats\n",
    "\n",
    "            # apply SWIR2 mask, calculate water stats:\n",
    "            msk = S2_xarray[dd].values\n",
    "            msk = (msk>(SWIR2_prc_thr*100.0))   # SWIR2_prc_thr * 10000.0 / 100.0\n",
    "            WQ_xarray.values[dd][msk] = np.nan\n",
    "            n_water_pix_swir = np.sum( ~np.isnan(WQ_xarray[dd].values) )\n",
    "\n",
    "            # low-water flag:\n",
    "            nan_pix_prct = 100.0*n_pix_poly_nan/n_pix_poly\n",
    "            water_pix_prct = 100.0*n_water_pix_swir/n_pix_poly\n",
    "            if nan_pix_prct<=(100.0-low_water_prc_thr):\n",
    "                if water_pix_prct<(low_water_prc_thr-nan_pix_prct):\n",
    "                    low_water_flag = 'true'\n",
    "                elif water_pix_prct>=low_water_prc_thr:\n",
    "                    low_water_flag_list[dd] = 'false'\n",
    "            #   else: low_water_flag_list[dd] = 'undefined'\n",
    "            else:\n",
    "                if water_pix_prct<(low_water_prc_thr-nan_pix_prct):\n",
    "                    low_water_flag_list[dd] = 'true'\n",
    "            #   else: low_water_flag_list[dd] = 'undefined'\n",
    "\n",
    "            # WOFS water flag:\n",
    "            if n_pix_wofs!=0:\n",
    "                if n_water_pix_swir>(n_pix_wofs+n_pix_wofs_nan):\n",
    "                    wofs_flag_list[dd] = \"perennial: larger than WOFS {}% extents\".format(wofs_prc_thr)\n",
    "                elif n_water_pix_swir<(n_pix_wofs-n_pix_wofs_nan):\n",
    "                    wofs_flag_list[dd] = \"perennial: smaller than WOFS {}% extents\".format(wofs_prc_thr)\n",
    "                else:\n",
    "                    wofs_flag_list[dd] = \"perennial: undefined\"\n",
    "        #---end: for dd in range(n_dates)\n",
    "\n",
    "\n",
    "        ### Remove \"empty\" time slices (ie. nr of valid pixels below threshold):\n",
    "        tmp = ( ~np.isnan(WQ_xarray) ).sum('x').sum('y').values\n",
    "        idx = np.where( tmp>=n_valid_pix_min )[0]\n",
    "        n_dates = len(idx)\n",
    "        out_log_str += \"    {} time slices left following SWIR2 masking.\".format(n_dates) + '\\n'\n",
    "        if n_dates==0:\n",
    "            out_log_str += \"    --> No data left for this satellite.\" + '\\n'\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[idx]\n",
    "        low_water_flag_list = [low_water_flag_list[ii] for ii in idx]\n",
    "        wofs_flag_list = [wofs_flag_list[ii] for ii in idx]\n",
    "\n",
    "\n",
    "        ### Process time series: apply buffered-polygon to WQ\n",
    "        for dd in range(n_dates): \n",
    "            WQ_xarray.values[dd][poly_mask_xy_bufd] = np.nan\n",
    "\n",
    "\n",
    "        ### Remove \"empty\" time slices (ie. nr of valid pixels below threshold):\n",
    "        tmp = ( ~np.isnan(WQ_xarray) ).sum('x').sum('y').values\n",
    "        idx = np.where( tmp>=n_valid_pix_min )[0]\n",
    "        n_dates = len(idx)\n",
    "        out_log_str += \"    {} time slices left following buffered polygon masking.\".format(n_dates) + '\\n'\n",
    "        if n_dates==0:\n",
    "            out_log_str += \"    --> No data left for this satellite.\" + '\\n'\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[idx]\n",
    "        low_water_flag_list = [low_water_flag_list[ii] for ii in idx]\n",
    "        wofs_flag_list = [wofs_flag_list[ii] for ii in idx]\n",
    "\n",
    "\n",
    "        ### Concatenate dataset with overall time series:\n",
    "        sat_flag_list = [plat_str for ii in range(n_dates)]\n",
    "        if concat_init:\n",
    "            # initialise variables\n",
    "            concat_init = False\n",
    "            WQ_xarray_TS = WQ_xarray.copy()\n",
    "            sat_flag_list_TS = sat_flag_list[:]   # list copy\n",
    "            low_water_flag_list_TS = low_water_flag_list[:]\n",
    "            wofs_flag_list_TS = wofs_flag_list[:]\n",
    "        else:\n",
    "            # concatenate dates\n",
    "            concat_dates = np.concatenate( (WQ_xarray_TS['time'].values, WQ_xarray['time'].values) )\n",
    "            sort_res = np.argsort(concat_dates)\n",
    "\n",
    "            # concatenate flags\n",
    "            tmp = sat_flag_list_TS + sat_flag_list   # list concatenation\n",
    "            sat_flag_list_TS = [tmp[ii] for ii in sort_res]\n",
    "            tmp = low_water_flag_list_TS + low_water_flag_list\n",
    "            low_water_flag_list_TS = [tmp[ii] for ii in sort_res]\n",
    "            tmp = wofs_flag_list_TS + wofs_flag_list\n",
    "            wofs_flag_list_TS = [tmp[ii] for ii in sort_res]\n",
    "\n",
    "            # concatenate data\n",
    "            sat1_idx = np.array( [ii for ii,ss in enumerate(sat_flag_list_TS) if ss!=plat_str] )\n",
    "            sat2_idx = np.array( [ii for ii,ss in enumerate(sat_flag_list_TS) if ss==plat_str] )\n",
    "            WQ_xarray_TS = xr.concat( [WQ_xarray_TS, WQ_xarray], dim='time', positions=[sat1_idx,sat2_idx] )\n",
    "        #---end: if concat_init\n",
    "    #---end: for sat in range(n_sats)\n",
    "    \n",
    "#     dc_api.datacube.index.close()\n",
    "    dc_api.datacube.close()\n",
    "    \n",
    "    if wofs_error:   # if error processing WOFS data, continue to next polygon\n",
    "        return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "\n",
    "    if concat_init:   # no data available/left for this polygon / ROI\n",
    "        out_log_str += \"  There is no valid data available for this polygon/ROI.\" + '\\n'\n",
    "        \n",
    "        if not last_proc_date==datetime(1,1,1):\n",
    "            with open(nodata_log_name,\"w\") as logf:   # print info to a \"no-data\" log file\n",
    "                logf.write( \"Name {}\\n\".format(lakes_name_list[ftr]) )\n",
    "                logf.write( \"DisplayName {}\\n\".format(lakes_dispname_list[ftr]) )\n",
    "                logf.write( \"ResultsVersion {}\\n\".format(results_version) )\n",
    "                logf.write( \"LastProcDate {}\\n\".format(str(last_proc_date)[:10]) )  # dataset processed up to that date\n",
    "            out_log_str += \"  Saved ROI's last processed date to log file '{}'.\".format(nodata_log_name) + '\\n'\n",
    "            \n",
    "        out_log_str += \"  (Polygon area is {} by {} pixels)\".format(npix_x,npix_y) + '\\n'\n",
    "        return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "\n",
    "    n_dates = WQ_xarray_TS.shape[0]\n",
    "    out_log_str += \"  There are {} time slices in the resulting multi-sensor time series.\".format(n_dates) + '\\n'\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: reprojecting to geodetic\n",
    "    ###########################################################################\n",
    "\n",
    "    # Reproject each time slice to geodetic:\n",
    "    npix_lon = len(lon_vec)\n",
    "    npix_lat = len(lat_vec)\n",
    "    n_dates, npix_y, npix_x = WQ_xarray_TS.shape\n",
    "    WQ_array_TS_geo = np.ones( (n_dates, npix_lat, npix_lon) ) * np.nan\n",
    "\n",
    "    for dd in range(n_dates):\n",
    "        # WQ gdal dataset:\n",
    "        gdal_data = gdal.GetDriverByName( 'MEM' ).Create('', npix_x, npix_y, 1, gdal.GDT_Float32)\n",
    "        gdal_data.SetGeoTransform( sat_geotx )\n",
    "        gdal_data.SetProjection( sat_proj.ExportToWkt() )\n",
    "        gdal_data.GetRasterBand(1).WriteArray( WQ_xarray_TS[dd].values, 0, 0)\n",
    "\n",
    "        # reprojected WQ gdal dataset:\n",
    "        gdal_proj_data = gdal.GetDriverByName( 'MEM' ).Create('', npix_lon, npix_lat, 1, gdal.GDT_Float32)\n",
    "        gdal_proj_data.SetGeoTransform( (lon_vec[0], pix_size_geo, 0, lat_vec[-1], 0, -pix_size_geo) )\n",
    "        gdal_proj_data.SetProjection( geo_proj.ExportToWkt() )\n",
    "\n",
    "        gdal_proj_data.GetRasterBand(1).WriteArray( np.ones((npix_lat,npix_lon))*np.nan, 0, 0)\n",
    "        tmp = gdal.ReprojectImage( gdal_data, gdal_proj_data,\n",
    "                                   sat_proj.ExportToWkt(), geo_proj.ExportToWkt(),\n",
    "                                   gdal.GRA_NearestNeighbour)\n",
    "                                   # gdal.GRA_Bilinear )   # found to eat away at edges of lake boundaries!...\n",
    "        WQ_array_TS_geo[dd] = gdal_proj_data.ReadAsArray()\n",
    "    #---end: for dd in range(n_dates)\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iterating over polygons: Lake stats & Green-Amber-Red flags\n",
    "    ###########################################################################\n",
    "\n",
    "    mean_vec = np.nanmean( WQ_array_TS_geo, (1,2) )\n",
    "    med_vec = np.nanmedian( WQ_array_TS_geo, (1,2) )\n",
    "    max_vec = np.nanmax( WQ_array_TS_geo, (1,2) )\n",
    "    min_vec = np.nanmin( WQ_array_TS_geo, (1,2) )\n",
    "\n",
    "    if np.size(alert_thresholds)==1:   # np.nan\n",
    "        GAR_flag_list = ['undefined' for ii in range(n_dates)]\n",
    "    else:\n",
    "        amb_idx = np.where( np.nansum( WQ_array_TS_geo>alert_thresholds[0], (1,2) )>0 )[0]\n",
    "        red_idx = np.where( np.nansum( WQ_array_TS_geo>alert_thresholds[1], (1,2) )>0 )[0]\n",
    "        GAR_flag_list = ['green' for ii in range(n_dates)]\n",
    "        for ii in amb_idx: GAR_flag_list[ii] = \"amber\"\n",
    "        for ii in red_idx: GAR_flag_list[ii] = \"red\"\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iterating over polygons: CPU info\n",
    "    ###########################################################################\n",
    "\n",
    "    poly_t1 = timer.time()\n",
    "    delta_t = poly_t1 - poly_t0\n",
    "\n",
    "    lake_area_ha = shape_list[ftr]/10000.0\n",
    "    box_area_ha = len(sat_xvec) * len(sat_yvec) * (sat_pix_size**2) / 10000.0\n",
    "    window_days = (roi_end_date - roi_start_date).days\n",
    "\n",
    "    # saving CPU data for later analysis:\n",
    "    out_cpu_arr[0,0] = delta_t\n",
    "    out_cpu_arr[0,1] = lake_area_ha\n",
    "    out_cpu_arr[0,2] = box_area_ha\n",
    "    out_cpu_arr[0,3] = window_days\n",
    "    out_cpu_arr[0,4] = n_dates_orig\n",
    "    out_cpu_arr[0,5] = n_dates\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: saving to NetCDF\n",
    "    ###########################################################################\n",
    "    \n",
    "    # Extracting dates in datetime format:\n",
    "    dt_dates = np.zeros(n_dates).astype(datetime)\n",
    "    for ii in range(n_dates):\n",
    "        tmp = str( WQ_xarray_TS['time'].values[ii] )\n",
    "        dt_dates[ii] = datetime( int(tmp[0:4]), int(tmp[5:7]), int(tmp[8:10]), int(tmp[11:13]), \n",
    "                                 int(tmp[14:16]), int(tmp[17:19]), int(tmp[20:26]) )\n",
    "    dt_dates = date2num(dt_dates, units=netcdf_time_units, calendar=netcdf_time_calendar )\n",
    "\n",
    "    if roi_append_nc:   # append to existing NetCDF file\n",
    "\n",
    "        roi_nc_grp = Dataset(roi_save_name, mode='a')\n",
    "        n_ex_dates = roi_nc_grp.dimensions['time'].size   # nr of existing dates\n",
    "        idx = np.arange(n_ex_dates, n_ex_dates+n_dates)\n",
    "\n",
    "        # load and append to variables:\n",
    "        wq_vals = roi_nc_grp.variables['WQ_data']\n",
    "        wq_vals[idx,:,:] = WQ_array_TS_geo\n",
    "\n",
    "        times = roi_nc_grp.variables['time']\n",
    "        times[idx] = dt_dates\n",
    "\n",
    "        mean_vals = roi_nc_grp.variables['WQ_means']\n",
    "        mean_vals[idx] = mean_vec\n",
    "\n",
    "        med_vals = roi_nc_grp.variables['WQ_medians']\n",
    "        med_vals[idx] = med_vec\n",
    "\n",
    "        max_vals = roi_nc_grp.variables['WQ_maxvals']\n",
    "        max_vals[idx] = max_vec\n",
    "\n",
    "        min_vals = roi_nc_grp.variables['WQ_minvals']\n",
    "        min_vals[idx] = min_vec\n",
    "\n",
    "        sat_vals = roi_nc_grp.variables['satellite_flags']\n",
    "        sat_vals[idx] = np.array(sat_flag_list_TS)\n",
    "\n",
    "        water_vals = roi_nc_grp.variables['low_water_flags']\n",
    "        water_vals[idx] = np.array(low_water_flag_list_TS)\n",
    "\n",
    "        wofs_vals = roi_nc_grp.variables['wofs_flags']\n",
    "        wofs_vals[idx] = np.array(wofs_flag_list_TS)\n",
    "\n",
    "        gar_vals = roi_nc_grp.variables['GAR_flags']\n",
    "        gar_vals[idx] = np.array(GAR_flag_list)\n",
    "\n",
    "        roi_nc_grp.LastProcDate = str(last_proc_date)[:10]  # dataset processed up to that date\n",
    "        roi_nc_grp.DateLastUpdated = datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
    "        roi_nc_grp.LastUpdateCPUinfo = (delta_t, lake_area_ha, box_area_ha, window_days, n_dates_orig, n_dates)\n",
    "\n",
    "        roi_nc_grp.close()\n",
    "        out_log_str += \"  WQ and ancillary data appended to file '{}'.\".format(roi_save_name) + '\\n'\n",
    "        \n",
    "    else:   # create new or replace existing NetCDF file\n",
    "\n",
    "        # backup the existing NetCDF file if desired\n",
    "        if roi_file_exists & backup_if_replacing_nc:\n",
    "            # create backup directory:\n",
    "            bak_dir_path = save_dir_path + 'backup/'\n",
    "            if not os.path.exists( bak_dir_path ):\n",
    "                os.makedirs( bak_dir_path )\n",
    "                if not os.path.exists( bak_dir_path ):\n",
    "                    out_log_str += \"  Error: could not create backup directory '{}'.\".format(bak_dir_path) + '\\n'\n",
    "                    return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "                out_log_str += \"  Created backup directory '{}'.\".format(bak_dir_path) + '\\n'\n",
    "\n",
    "            # create backup:\n",
    "            tmp = bak_dir_path+roi_file_str\n",
    "            os.rename(roi_save_name, tmp)\n",
    "            if not os.path.isfile( tmp ):\n",
    "                out_log_str += \"  Error: could not back up NetCDF file '{}'.\".format(tmp) + '\\n'\n",
    "                return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "            out_log_str += \"  Existing NetCDF file backed up to '{}'.\".format(tmp) + '\\n'\n",
    "        #---end if\n",
    "\n",
    "        # open NetCDF dataset:\n",
    "        roi_nc_grp = Dataset(roi_save_name, mode='w')   # overwrites existing file\n",
    "\n",
    "        # create NetCDF dimensions\n",
    "        time = roi_nc_grp.createDimension(\"time\", None)   # unlimited dimension\n",
    "        lat = roi_nc_grp.createDimension(\"lat\", len(lat_vec) )\n",
    "        lon = roi_nc_grp.createDimension(\"lon\", len(lon_vec) )\n",
    "\n",
    "        # create NetCDF variables\n",
    "        times = roi_nc_grp.createVariable(\"time\",\"f8\",(\"time\",))\n",
    "        lats = roi_nc_grp.createVariable(\"lat\",\"f8\",(\"lat\",))\n",
    "        lons = roi_nc_grp.createVariable(\"lon\",\"f8\",(\"lon\",))\n",
    "\n",
    "        wq_vals = roi_nc_grp.createVariable(\"WQ_data\",\"f8\",(\"time\",\"lat\",\"lon\",))\n",
    "        mean_vals = roi_nc_grp.createVariable(\"WQ_means\",\"f8\",(\"time\",))\n",
    "        med_vals = roi_nc_grp.createVariable(\"WQ_medians\",\"f8\",(\"time\",))\n",
    "        max_vals = roi_nc_grp.createVariable(\"WQ_maxvals\",\"f8\",(\"time\",))\n",
    "        min_vals = roi_nc_grp.createVariable(\"WQ_minvals\",\"f8\",(\"time\",))\n",
    "\n",
    "        sat_vals = roi_nc_grp.createVariable(\"satellite_flags\",\"S32\",(\"time\",))   # Define str length long enough for flags!\n",
    "        water_vals = roi_nc_grp.createVariable(\"low_water_flags\",\"S16\",(\"time\",))   # Define str length long enough for flags!\n",
    "        wofs_vals = roi_nc_grp.createVariable(\"wofs_flags\",\"S64\",(\"time\",))   # Define str length long enough for flags!\n",
    "        gar_vals = roi_nc_grp.createVariable(\"GAR_flags\",\"S8\",(\"time\",))   # Define str length long enough for flags!\n",
    "\n",
    "        # assign values to variables\n",
    "        times.units = netcdf_time_units\n",
    "        times.calendar = netcdf_time_calendar\n",
    "        times[:] = dt_dates\n",
    "        lons[:] = lon_vec\n",
    "        lats[:] = lat_vec[::-1]\n",
    "\n",
    "        wq_vals[:,:,:] = WQ_array_TS_geo\n",
    "        mean_vals[:] = mean_vec\n",
    "        med_vals[:] = med_vec\n",
    "        max_vals[:] = max_vec\n",
    "        min_vals[:] = min_vec\n",
    "\n",
    "        sat_vals[:] = np.array(sat_flag_list_TS)\n",
    "        water_vals[:] = np.array(low_water_flag_list_TS)\n",
    "        wofs_vals[:] = np.array(wofs_flag_list_TS)\n",
    "        gar_vals[:] = np.array(GAR_flag_list)\n",
    "\n",
    "        # dataset attributes:\n",
    "        roi_nc_grp.Name = lakes_name_list[ftr]\n",
    "        roi_nc_grp.DisplayName = lakes_dispname_list[ftr]\n",
    "        roi_nc_grp.EPSG = geo_proj.GetAttrValue(\"AUTHORITY\", 1)\n",
    "        roi_nc_grp.LakeType = \"undefined\"   # clear/deep vs. turbid/shallow ... should be defined somehow!\n",
    "        roi_nc_grp.WQtype = WQ_type\n",
    "        roi_nc_grp.GAR_thresholds = alert_thresholds\n",
    "        roi_nc_grp.LastProcDate = str(last_proc_date)[:10]  # dataset processed up to that date\n",
    "        roi_nc_grp.DateCreated = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        roi_nc_grp.DateLastUpdated = \"NA\"\n",
    "        roi_nc_grp.LastUpdateCPUinfo = (delta_t, lake_area_ha, box_area_ha, window_days, n_dates_orig, n_dates)\n",
    "        roi_nc_grp.ResultsVersion = results_version\n",
    "\n",
    "        roi_nc_grp.close()\n",
    "\n",
    "        if not os.path.isfile( roi_save_name ):\n",
    "            out_log_str += \"  Error: could not create NetCDF file '{}'.\".format(roi_save_name) + '\\n'\n",
    "            return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "        out_log_str += \"  WQ and ancillary data saved to file '{}'.\".format(roi_save_name) + '\\n'\n",
    "    #---end if append_to_existing_nc\n",
    "    \n",
    "    out_nc_flag = True\n",
    "    \n",
    "    if nodata_file_exists:\n",
    "        os.remove(nodata_log_name)\n",
    "        out_log_str += \"  Deleted \\\"no-data\\\" file '{}'.\".format(nodata_log_name) + '\\n'\n",
    "\n",
    "    # Log info:\n",
    "    tmp = delta_t / lake_area_ha\n",
    "    out_log_str += \"  Total time processing polygon (h:m:s): {}\".format(str(timedelta(seconds=delta_t))) + '\\n'\n",
    "    out_log_str += \"  (Polygon area is {} by {} pixels)\".format(npix_x,npix_y) + '\\n'\n",
    "    \n",
    "    return [out_log_str, out_cpu_arr, out_nc_flag]\n",
    "#---end: def ftr_loop(ftr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lakes_shp_file = '../NSW_lakes/NSW_WaterBody.shp'\n",
    "   # shape file containing polygons for waterbodies of interest.\n",
    "lake_min_area = 5000\n",
    "   # waterbody screening: minimum area size in sq.m. for a waterbody to be considered.\n",
    "lake_screen_perennial = True\n",
    "   # waterbody screening flag: keep only perennial waterbodies?\n",
    "polygon_buffer_width = 1.5\n",
    "   # buffering of lake edges: distance (in Landsat pixels) from lake edges where pixels are masked out\n",
    "lake_min_valid_prc = 10.0\n",
    "   # minimum percentage of valid (non-NaN) extent required for lake to be processed / saved\n",
    "\n",
    "pix_size_geo = 0.00025   \n",
    "   # reprojection to geodetic: desired geodetic pixel size\n",
    "lake_buffer_width_geo = 0.002  \n",
    "   # buffer width to add around lake polygon extents for processing, in geodetic units\n",
    "\n",
    "cloud_buffer_width = 12.0\n",
    "   # buffer width (in Landsat pixels) to add around cloud and cloud shadow pixels in cloud mask\n",
    "\n",
    "results_version = 'v3.1'\n",
    "   # metadata version nr of current WQ outputs\n",
    "save_dir_path = \"/g/data/jr4/vis_data_v3.1/\"\n",
    "   # base directory path where the WQ outputs are saved: will be created if necessary; can contain existing data.\n",
    "append_to_existing_nc = False  #True\n",
    "   # flag: append to dataset on file if exists; otherwise, overwrite it (i.e. create new file).\n",
    "backup_if_replacing_nc = True\n",
    "   # flag: if existing dataset is overwritten, create backup first.\n",
    "    \n",
    "start_date = '2015-01-01'   # '2006-01-01'\n",
    "   # start date of time window, format YYYY-MM-DD; will be determined from .nc file if exists and if appending\n",
    "end_date = None   # '2015-12-31'  # '2014-01-01'   # None\n",
    "   # end date, same format: if set to None, will use current date as end date; can be same as 'start_date'\n",
    "\n",
    "wofs_dir_path = '/g/data2/fk4/wofs/current/pyramids/WaterSummary/0/'   \n",
    "   # base path where geoTiff WOFS data is located\n",
    "wofs_prc_thr = 90.0\n",
    "   # percentage threshold for WOFS water mask\n",
    "\n",
    "SWIR2_prc_thr = 1.0\n",
    "   # percentage threshold for SWIR2 water mask\n",
    "low_water_prc_thr = 50.0\n",
    "   # percentage threshold below which the low-water flag is triggered (compared to polygon extents)\n",
    "\n",
    "WQ_type = 'Alg2:TwoBand_TSS_calib'   # 'Alg1:(red+green)/2'\n",
    "   # (string) type of water quality algorithm to use to generate results\n",
    "alert_thresholds = None   # e.g. (2000,2400)\n",
    "   # red/amber/green thresholds, defined as tuple (green_to_amber,amber_to_red); None if undefined\n",
    "    \n",
    "netcdf_time_units = \"seconds since 1970-01-01 00:00:00.0\"\n",
    "   # time units to use when saving data to NetCDF format\n",
    "netcdf_time_calendar = \"standard\"\n",
    "   # calendar to use when saving data to NetCDF format\n",
    "\n",
    "no_screen_outputs = True #False   #True\n",
    "   # disables all printing to screen (e.g. when processing all 900 odd lakes)\n",
    "num_threads = 12  #None\n",
    "   # selected nr of threads for parallel processing; set to 'None' to set to all available CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of satellites whose dataset will be used for WQ processing. These definitions need \n",
    "# to be determined on the basis of the current AGDC dataset (module) used.\n",
    "\n",
    "# Landsat 8 definitions:\n",
    "sat_1 = { 'API_sat_str' : 'ls8_nbar_albers',  # satellite string for API queries\n",
    "          'API_plat_str' : 'LANDSAT_8',       # satellite platform string for API queries\n",
    "          'Alg2_calib_fac' : 3957.0,          # calibration constant (multiplicative factor) for selected WQ algorithm\n",
    "          'Alg2_calib_exp' : 1.6436 }         # calibration constant (exponent) for selected WQ algorithm\n",
    "\n",
    "# Landsat 7 definitions:\n",
    "sat_2 = { 'API_sat_str' : 'ls7_nbar_albers',\n",
    "          'API_plat_str' : 'LANDSAT_7',\n",
    "          'Alg2_calib_fac' : 3983.0,\n",
    "          'Alg2_calib_exp' : 1.6246 }\n",
    "\n",
    "# Landsat 5 definitions:\n",
    "sat_3 = { 'API_sat_str' : 'ls5_nbar_albers',\n",
    "          'API_plat_str' : 'LANDSAT_5',\n",
    "          'Alg2_calib_fac' : 3983.0,\n",
    "          'Alg2_calib_exp' : 1.6246 }\n",
    "\n",
    "WQ_sats = { 'sat1' : sat_1,   # resulting dictionary of all satellite defs to use\n",
    "            'sat2' : sat_2,\n",
    "            'sat3' : sat_3 }\n",
    "\n",
    "# API definitions, (currently) valid for all satellites:\n",
    "API_defs = { 'nbar_product_str' : 'nbar',      # NBAR product string\n",
    "             'red_band_str' : 'red',           # NBAR red band string\n",
    "             'green_band_str' : 'green',       # NBAR green band string\n",
    "             'swir2_band_str' : 'swir2',       # NBAR swir2 band string\n",
    "             'pq_product_str' : 'pqa',         # PQ product string\n",
    "             'pq_band_str' : 'pixelquality',   # PQ band string\n",
    "             'green_satur_bit' : 1,        # PQ saturation bit for green band\n",
    "             'red_satur_bit' : 2,          # PQ saturation bit for red band\n",
    "             'swir2_satur_bit' : 7,        # PQ saturation bit for SWIR2 band\n",
    "             'contig_bit' : 8,      # PQ contiguity bit \n",
    "             'land_bit' : 9,        # PQ land/sea bit\n",
    "             'cloud_bit1' : 10,     # PQ cloud/shadow bits\n",
    "             'cloud_bit2' : 11,\n",
    "             'cloud_bit3' : 12,\n",
    "             'cloud_bit4' : 13 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Projection to use for final maps: here using geodetic, could be user-specific (would require some re-coding)\n",
    "geo_proj = osr.SpatialReference ()\n",
    "geo_proj.ImportFromEPSG ( 4326 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if alert_thresholds==None:\n",
    "    alert_thresholds = np.nan\n",
    "else:\n",
    "    alert_thresholds = np.array(alert_thresholds)\n",
    "    if len(alert_thresholds)!=2 or alert_thresholds[0]>=alert_thresholds[1]:\n",
    "        raise RuntimeError(\"Alert thresholds not defined properly (2 values in increasing order).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create directories if necessary:\n",
    "if not wofs_dir_path.endswith(\"/\"):\n",
    "    wofs_dir_path = wofs_dir_path + \"/\"\n",
    "if not save_dir_path.endswith(\"/\"):\n",
    "    save_dir_path = save_dir_path + \"/\"\n",
    "    \n",
    "# Directory where some log info is saved for polygons without any valid data (avoids future re-processing)\n",
    "nodata_dir_path = save_dir_path + \"no_data/\"\n",
    "\n",
    "# log file:\n",
    "log_base_path = save_dir_path + \"WQproc_\" + datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if not os.path.exists( save_dir_path ):\n",
    "    os.makedirs( save_dir_path )\n",
    "    if not os.path.exists( save_dir_path ):\n",
    "        raise RuntimeError(\"Could not create directory '{}'.\".format(save_dir_path) )\n",
    "    log_file = open(log_base_path + \".log\", \"w\")\n",
    "    write_to_log( log_file, \"Created results directory '{}'.\".format(save_dir_path) )\n",
    "else:\n",
    "    log_file = open(log_base_path + \".log\", \"w\")\n",
    "\n",
    "if not os.path.exists( nodata_dir_path ):\n",
    "    os.makedirs( nodata_dir_path )\n",
    "    if not os.path.exists( nodata_dir_path ):\n",
    "        raise RuntimeError(\"Could not create directory '{}'.\".format(nodata_dir_path) )\n",
    "    write_to_log( log_file, \"Created \\\"no-data\\\" directory '{}'.\".format(nodata_dir_path) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AGDC defs:\n",
    "# dc_api = datacube.api.API()\n",
    "dc_dc = datacube.Datacube()\n",
    "# ae = AnalyticsEngine()\n",
    "# ee = ExecutionEngine()\n",
    "dc_prods = dc_dc.list_products()\n",
    "\n",
    "pqbnd = API_defs['pq_band_str']\n",
    "nbar_prod = API_defs['nbar_product_str']\n",
    "pq_prod = API_defs['pq_product_str']\n",
    "rbnd = API_defs['red_band_str']\n",
    "gbnd = API_defs['green_band_str']\n",
    "s2bnd = API_defs['swir2_band_str']\n",
    "\n",
    "contig_bit = API_defs['contig_bit']\n",
    "land_bit = API_defs['land_bit']\n",
    "swir2_sat_bit = API_defs['swir2_satur_bit']\n",
    "red_sat_bit = API_defs['red_satur_bit']\n",
    "green_sat_bit = API_defs['green_satur_bit']\n",
    "other_bit_mask = ( 1<<green_sat_bit | 1<<red_sat_bit | 1<<swir2_sat_bit | 1<<contig_bit | 1<<land_bit )\n",
    "\n",
    "cloud_bit1 = API_defs['cloud_bit1']\n",
    "cloud_bit2 = API_defs['cloud_bit2']\n",
    "cloud_bit3 = API_defs['cloud_bit3']\n",
    "cloud_bit4 = API_defs['cloud_bit4']\n",
    "cloud_bit_mask = (1<<cloud_bit1 | 1<<cloud_bit2 | 1<<cloud_bit3 | 1<<cloud_bit4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading waterbody polygons from `.shp` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open .shp file:\n",
    "lakes_vec = ogr.Open( lakes_shp_file )\n",
    "if lakes_vec==None:\n",
    "    raise RuntimeError( \"Cannot open shape file '{}'.\".format(lakes_shp_file) )\n",
    "if lakes_vec.GetLayerCount()!=1:\n",
    "    raise RuntimeError( \"File '{}' has 0 or multiple layers (1 expected).\".format(lakes_shp_file) )\n",
    "lakes_lyr = lakes_vec.GetLayer(0)\n",
    "\n",
    "# List of waterbody names and attributes:\n",
    "n_ftr = lakes_lyr.GetFeatureCount()\n",
    "lname_list = list()\n",
    "peren_list = list()\n",
    "shape_list = list()\n",
    "for kk in range( n_ftr ):\n",
    "    ftr = lakes_lyr.GetFeature(kk)\n",
    "    lname_list.append(ftr.GetField('hydroname'))   # name\n",
    "    peren_list.append(ftr.GetField('perennia_1'))   # perennial flag\n",
    "    shape_list.append(ftr.GetField('shape_STAr'))   # area in sq.m.\n",
    "\n",
    "# Screen waterbodies of interest:\n",
    "tmp = ( np.array(shape_list)>=lake_min_area )\n",
    "if lake_screen_perennial: \n",
    "    tmp = ( (np.array(peren_list)=='Perennial') & tmp )\n",
    "lakes_ftr_idx = np.where(tmp)[0]\n",
    "n_lakes = len(lakes_ftr_idx)\n",
    "if n_lakes==0:\n",
    "    raise RuntimeError( \"No waterbody left to process after screening.\")\n",
    "lakes_name_list = [ lname_list[ii] for ii in lakes_ftr_idx ]\n",
    "shape_list = [ shape_list[ii] for ii in lakes_ftr_idx ]\n",
    "lakes_dispname_list = lakes_name_list[:]   # list copy\n",
    "\n",
    "# Check for duplicated names, rename them (append _0, _1, etc.):\n",
    "renamed_cntr = 0\n",
    "for kk in range(n_lakes):\n",
    "    tmp = [ i for i,s in enumerate(lakes_name_list) if s==lakes_name_list[kk] ]\n",
    "    n_tmp = len(tmp)\n",
    "    if n_tmp>1:\n",
    "        renamed_cntr = renamed_cntr + 1\n",
    "        for ii,tt in enumerate(tmp):\n",
    "            lakes_name_list[tt] = lakes_name_list[tt] + \"_\" + str(ii)\n",
    "\n",
    "lakes_name_list = [ lakes_name_list[ii].replace(\" \",\"_\") for ii in range(n_lakes) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Log info:\n",
    "write_to_log( log_file, \"Shape file '{}' info:\".format(lakes_shp_file) )\n",
    "write_to_log( log_file, \"  There are {} waterbodies in the file '{}'.\".format(n_ftr,lakes_shp_file) )\n",
    "tmp = \"  A total of {} {}\".format(n_lakes,\"perennial lakes\" if lake_screen_perennial else \"lakes\")\n",
    "tmp = tmp + \"with area of more than {} sq.m. will be processed.\".format(lake_min_area)\n",
    "write_to_log( log_file, tmp )\n",
    "\n",
    "if renamed_cntr>0:\n",
    "    write_to_log( log_file, \"  A total of {} lake names were re-labelled to avoid duplicates.\".format(renamed_cntr) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration over polygons ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallel processing of waterbodies:\n",
    "n_cores = mp.cpu_count()\n",
    "if num_threads==None:\n",
    "    num_threads = n_cores\n",
    "\n",
    "pool = mp.Pool(processes=num_threads)\n",
    "\n",
    "poly_t_init = timer.time()\n",
    "\n",
    "write_to_log( log_file, '\\nCurrent system has {} available CPUs.'.format(n_cores) )\n",
    "write_to_log( log_file, 'Starting parallel processing of {} polygons using {} threads...'.format(n_lakes,num_threads) )\n",
    "\n",
    "ftr_idx = np.argsort(shape_list)[-1::-1]   # process larger lakes first (multi-processing)\n",
    "res = pool.map(ftr_loop, ftr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process mp outputs:\n",
    "n_nc_files = 0\n",
    "cpu_data = np.ones((n_lakes,6))*np.nan\n",
    "write_to_log( log_file, '' )\n",
    "\n",
    "tmp = np.argsort(ftr_idx)\n",
    "for ftr in tmp:\n",
    "    write_to_log( log_file, res[ftr][0] )   # out_log_str\n",
    "    cpu_data[ftr,:] = res[ftr][1]   # out_cpu_arr\n",
    "    n_nc_files += res[ftr][2]   # out_nc_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delta_t = timer.time() - poly_t_init\n",
    "tmp = str(timedelta(seconds=delta_t))\n",
    "write_to_log( log_file, \"\\nTotal time processing all {} polygons (h:m:s): {}\".format(n_lakes,tmp), True )\n",
    "write_to_log( log_file, \"A total of {} NetCDF files were created or updated.\".format(n_nc_files), True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment / analysis of computing times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcp['axes.formatter.useoffset'] = False\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "idx = np.where( ~np.isnan(cpu_data[:,0]) )[0]\n",
    "n_cpu = len(idx)\n",
    "\n",
    "if n_cpu==0:\n",
    "    write_to_log( log_file, \"\\nNo CPU times to analyse.\" )\n",
    "else:\n",
    "    cpu_data = cpu_data[idx,:]\n",
    "    Y = np.log( cpu_data[:,0] )\n",
    "    \n",
    "    with PdfPages(log_base_path + \".pdf\") as pdf:\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.hist(Y, bins=60)\n",
    "        plt.xlabel(\"log of proc. time (sec.)\")\n",
    "        plt.title( 'Proc. time histogram ({} polygons with available data)'.format(n_cpu) )\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = np.log(cpu_data[:,1]); plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"log of lake area (ha)\"); plt.title('Lake area (polygon) histogram')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('log of lake area (ha)'); plt.ylabel('log of proc. time (sec.)');\n",
    "        tmp = 'log_proc_time = {:.3g} * log_lake_area {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = np.power( np.log(cpu_data[:,1]), 3 ); plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"log of lake area (ha) cubed\"); plt.title('Lake area (polygon) histogram')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('log of lake area (ha) cubed'); plt.ylabel('log of proc. time (sec.)');\n",
    "        tmp = 'log_proc_time = {:.3g} * log_lake_area^3 {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = np.log(cpu_data[:,2]); plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"log of box area (ha)\"); plt.title('Bounding box area (polygon extents) histogram')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('log of box area (ha)'); plt.ylabel('log of proc. time (sec.)');\n",
    "        tmp = 'log_proc_time = {:.3g} * log_box_area {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = np.power( np.log(cpu_data[:,2]), 3 ); plt.hist(X, bins=60)\n",
    "        X_boxarea = X.copy()\n",
    "        plt.xlabel(\"log of box area (ha) cubed\"); plt.title('Bounding box area (polygon extents) histogram')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('log of box area (ha) cubed'); plt.ylabel('log of proc. time (sec.)');\n",
    "        tmp = 'log_proc_time = {:.3g} * log_box_area^3 {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,4]; plt.hist(X, bins=60)\n",
    "        X_nrdates = X.copy()\n",
    "        plt.xlabel(\"nr. of dates\"); plt.title('Original nr. of time slices (all available) histogram')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('nr. of dates'); plt.ylabel('log of proc. time (sec.)');\n",
    "        tmp = 'log_proc_time = {:.3g} * n_dates {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,5]; plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"nr. of dates\"); plt.title('Final nr. of time slices (final series) histogram')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('nr. of dates'); plt.ylabel('log of proc. time (sec.)');\n",
    "        tmp = 'log_proc_time = {:.3g} * n_dates {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "    write_to_log( log_file, \"\\nAssessment plots of CPU times saved to file '{}'.\".format(log_base_path + \".pdf\") )\n",
    "    \n",
    "    X_tmp = np.ones((len(X_boxarea),2))\n",
    "    X_tmp[:,0] = X_boxarea\n",
    "    X_tmp[:,1] = X_nrdates\n",
    "    reg.fit( np.matrix(X_tmp), np.matrix(Y).T )\n",
    "    tmp = '  Predicted proc. time per polygon:\\n'\n",
    "    tmp += '    log_proc_time = {:.3g} * log_box_area^3 {:+.3g} * n_dates_orig {:+.3g}  (R^2 = {:.3g})'\n",
    "    tmp = tmp.format( reg.coef_[0][0], reg.coef_[0][1], reg.intercept_[0], reg.score(np.matrix(X_tmp),np.matrix(Y).T) )\n",
    "    write_to_log( log_file, tmp )\n",
    "\n",
    "    tmp = log_base_path + \".pckl\"\n",
    "    with open(tmp, 'wb') as ff:\n",
    "        pickle.dump(cpu_data, ff)\n",
    "    write_to_log( log_file, \"CPU times saved to file '{}'.\".format(tmp) )\n",
    "#---end: if n_cpu==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "write_to_log( log_file, \"\\n+++ execution end +++\\n\")\n",
    "log_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
