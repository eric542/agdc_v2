{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AEWS Python Notebook 09d: complete (pre-batch) WQ algorithm\n",
    "\n",
    "**Author**: Eric Lehmann, CSIRO Data61  \n",
    "**Date**: Aug. 11, 2016.\n",
    "\n",
    "**Note**: this notebook should be accessible and viewable at https://github.com/eric542/agdc_v2/tree/master/notebooks.\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook brings together all the AEWS components tested / implemented in earlier notebooks. It implements the whole AEWS workflow, starting from the selected ROI and AGDC data, and ultimately creating a NetCDF dataset containing a time series of WQ maps with associated ancillary information. For testing purposes, this is here carried out for a single selected polygon, and thus represents a pre-batch version. A fully automated batch script executed on the NCI will automatically iterate through all the polygons of interest.\n",
    "\n",
    "This notebook version (09d) is a copy of '_AEWS Python Notebook 09c_', executed using the post-release AGDC v2.0 API. In particular, the following aspects have been updated compared to the earlier notebook (09c):\n",
    "\n",
    "+ conversion of UTC dates to solar days\n",
    "+ use of new/updated (post-release) API functions and definitions\n",
    "\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "This (Jupyter) notebook was written for use on the NCI's VDI system, with the following pre-loaded module:\n",
    "\n",
    "```\n",
    " $ module use /g/data/v10/public/modules/modulefiles --append\n",
    " $ module load agdc-py2-prod \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ogr, osr, gdal\n",
    "import xarray as xr\n",
    "import os\n",
    "\n",
    "from netCDF4 import Dataset, num2date, date2num\n",
    "from datetime import timedelta, datetime    # date\n",
    "\n",
    "from matplotlib.path import Path   # for point-in-polygon\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import time as timer\n",
    "from pprint import pprint\n",
    "from __future__ import print_function\n",
    "\n",
    "import datacube.api\n",
    "from datacube.analytics.analytics_engine import AnalyticsEngine\n",
    "from datacube.execution.execution_engine import ExecutionEngine\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams as rcp\n",
    "\n",
    "from sklearn import linear_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_mask(mask_arr, npix):\n",
    "    # Uses the True/False (masked/non-masked) values in the array 'mask_arr' and \n",
    "    # expands the True values spatially by 'npix' pixels. The value 'npix' can be\n",
    "    # non-integer, i.e. the mask can be expanded by any spatial distance.\n",
    "    nmid = np.floor(npix)\n",
    "    nmax = int( nmid*2 + 1 )\n",
    "    struc = np.zeros((nmax, nmax), dtype='bool')\n",
    "    for ii in range(nmax):   # create desired binary structure for morphological operation\n",
    "        for jj in range(ii,nmax):\n",
    "            if pdist( [[nmid,nmid], [ii,jj]] ) <= npix:\n",
    "                struc[ii,jj] = True\n",
    "                struc[jj,ii] = True\n",
    "    return ndimage.binary_dilation(mask_arr, structure=struc)\n",
    "\n",
    "\n",
    "def merge_dup_dates(xrda):\n",
    "    # Takes in an xarray.DataArray 'xrda' as input and merges \n",
    "    # datasets (time slices) within it that have the same dates.\n",
    "    # Returns the modified DataArray and vector of selected dates.\n",
    "    dates = xrda.coords['time'].values\n",
    "    n_dates = len( dates )\n",
    "    \n",
    "    # Convert UTM times to local dates (d/m/y only):\n",
    "    str_dates = np.zeros(n_dates).astype('str')\n",
    "    for ii in range(n_dates):\n",
    "        str_dates[ii] = str( dates[ii] )[:10]\n",
    "    \n",
    "    # Remove duplicated dates:\n",
    "    rem_ind = np.zeros(n_dates).astype('bool')   # keep track of which duplicated dates to remove\n",
    "    for ind in range(1,n_dates):\n",
    "        dup_ind = np.where( str_dates[:ind]==str_dates[ind] )[0]   # check for duplicated date up to current index\n",
    "        if len( dup_ind )!=0:   # found (at least) one duplicate\n",
    "            dup_ind = dup_ind[0]   # only use the first index if multiple dates returned\n",
    "            rem_ind[ind] = True    # remove current date index\n",
    "            ind_n_nans = np.sum( np.isnan( xrda[ind] ) ) # nr of NaN pixels in each image\n",
    "            dup_n_nans = np.sum( np.isnan( xrda[dup_ind] ) )\n",
    "            if ind_n_nans==0:     # current data has no NaN's, use it instead of duplicate date (copy it to lowest index)\n",
    "                xrda[dup_ind] = xrda[ind]\n",
    "                xrda['time'].values[dup_ind] = xrda['time'].values[ind]\n",
    "            elif dup_n_nans!=0:   # if duplicate date has no NaN's: do nothing (use it instead of current date)\n",
    "                if dup_n_nans<ind_n_nans:   # duplicate date has less NaN's: fill it in with current data\n",
    "                    tmp = np.where( np.isnan(xrda[dup_ind]) )\n",
    "                    xrda[dup_ind].values[tmp] = xrda[ind].values[tmp]\n",
    "                else:   # dup_n_nans>=ind_n_nans -- duplicate date has more NaN's: use it to fill in current data\n",
    "                    tmp = np.where( np.isnan(xrda[ind]) )\n",
    "                    xrda[ind].values[tmp] = xrda[dup_ind].values[tmp]\n",
    "                    xrda[dup_ind] = xrda[ind]   # save results to lowest date index, in case >2 slices have same date\n",
    "                    xrda['time'].values[dup_ind] = xrda['time'].values[ind]\n",
    "    \n",
    "    return xrda[~rem_ind]\n",
    "\n",
    "\n",
    "def load_wofs_data(wofs_path, min_lon, max_lon, min_lat, max_lat):\n",
    "    # Returns array of WOFS data for given extents. Can read data from \n",
    "    # multiple adjacent tiles if necessary, at most 2 in either direction.\n",
    "    lon_idx1 = np.floor(min_lon); lon_idx2 = np.floor(max_lon)\n",
    "    dlon = abs(lon_idx1-lon_idx2)\n",
    "    lat_idx1 = np.floor(min_lat); lat_idx2 = np.floor(max_lat)\n",
    "    dlat = abs(lat_idx1-lat_idx2)\n",
    "    if ( dlon>1 or dlat>1 ):\n",
    "        raise RuntimeError(\"Lake extents span 3 or more WOFS tiles (in at least one dimension).\")\n",
    "    adj_tile_lon = (dlon==1)    # two adjacent 'horizontal' WOFS tiles\n",
    "    adj_tile_lat = (dlat==1)    # two adjacent 'vertical' WOFS tiles\n",
    "\n",
    "    # Load the first (top-left) of (potentially) several WOFS tiles:\n",
    "    WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(min_lon), np.floor(max_lat) )\n",
    "    WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "    if WOFS_dataset==None:\n",
    "        raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "    WOFS_array = WOFS_dataset.ReadAsArray()\n",
    "    WOFS_geotx = list( WOFS_dataset.GetGeoTransform() )\n",
    "    WOFS_proj = WOFS_dataset.GetProjection()   # geodetic lat/lon\n",
    "    # WOFS_srs = osr.SpatialReference( wkt=WOFS_proj )\n",
    "    WOFS_lonvec = np.arange(WOFS_array.shape[0]) * WOFS_geotx[1] + WOFS_geotx[0]\n",
    "    WOFS_latvec = np.arange(WOFS_array.shape[1]) * WOFS_geotx[5] + WOFS_geotx[3]\n",
    "    WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "    if ( adj_tile_lon ):   # if 2 horizontal tiles, load (top) right tile\n",
    "        WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(max_lon), np.floor(max_lat) )\n",
    "        WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "        if WOFS_dataset==None:\n",
    "            raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "        wa2 = WOFS_dataset.ReadAsArray()\n",
    "        wg2 = list( WOFS_dataset.GetGeoTransform() )\n",
    "        WOFS_lonvec = np.concatenate(( WOFS_lonvec, np.arange(wa2.shape[0]) * wg2[1] + wg2[0] ),0)\n",
    "        WOFS_array = np.concatenate((WOFS_array,wa2),1)   # column bind\n",
    "        WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "    if ( adj_tile_lat ):   # if 2 vertical tiles: load bottom (left) tile\n",
    "        WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(min_lon), np.floor(min_lat) )\n",
    "        WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "        if WOFS_dataset==None:\n",
    "            raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "        wa2 = WOFS_dataset.ReadAsArray()\n",
    "        wg2 = list( WOFS_dataset.GetGeoTransform() )\n",
    "        WOFS_latvec = np.concatenate(( WOFS_latvec, np.arange(wa2.shape[1]) * wg2[5] + wg2[3] ),0)\n",
    "        WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "        if ( adj_tile_lon ):   # if 2 horizontal & 2 vertical tiles: load bottom right tile\n",
    "            WOFS_fname = wofs_path + 'percentWater_{:3.0f}_{:04.0f}.tiff'.format( np.floor(max_lon), np.floor(min_lat) )\n",
    "            WOFS_dataset = gdal.Open(WOFS_fname)\n",
    "            if WOFS_dataset==None:\n",
    "                raise RuntimeError( \"Cannot open WOFS data file '{}'.\".format(WOFS_fname) )\n",
    "            wa3 = WOFS_dataset.ReadAsArray()\n",
    "            wa2 = np.concatenate((wa2,wa3),1)   # column bind\n",
    "            WOFS_dataset = None   # closes the gdal dataset\n",
    "\n",
    "        WOFS_array = np.concatenate((WOFS_array,wa2),0)   # row bind\n",
    "    \n",
    "    # Extract WOFS over region of interest:\n",
    "    yind_min = np.where( WOFS_lonvec>=min_lon )[0][0]\n",
    "    yind_max = np.where( WOFS_lonvec<=max_lon )[0][-1]\n",
    "    WOFS_lonvec = WOFS_lonvec[yind_min:yind_max+1]\n",
    "    xind_max = np.where( WOFS_latvec>=min_lat )[0][-1]\n",
    "    xind_min = np.where( WOFS_latvec<=max_lat )[0][0]\n",
    "    WOFS_latvec = WOFS_latvec[xind_min:xind_max+1]\n",
    "    WOFS_array = WOFS_array[xind_min:xind_max+1, yind_min:yind_max+1]\n",
    "    WOFS_geotx[0] = WOFS_geotx[0] + yind_min*WOFS_geotx[1]\n",
    "    WOFS_geotx[3] = WOFS_geotx[3] + xind_min*WOFS_geotx[5]\n",
    "\n",
    "    return WOFS_array, WOFS_proj, WOFS_geotx, WOFS_lonvec, WOFS_latvec\n",
    "\n",
    "\n",
    "def solar_day(utc, lon):\n",
    "    # returns approx. solar data from 'utc' time and average 'lon'.\n",
    "    # Based on GA's implementation, available at:\n",
    "    # https://github.com/data-cube/agdc-v2/blob/38a3430b3f49977f3b07355ba0d5d2d4ad7bf965/datacube/api/geo_xarray.py\n",
    "    seconds_per_degree = 240\n",
    "    offset_seconds = int(lon * seconds_per_degree)\n",
    "    offset = np.timedelta64(offset_seconds, 's')\n",
    "    return utc + offset\n",
    "\n",
    "\n",
    "def write_to_log(logf,strn):\n",
    "    # Prints info 'strn' to command line as well as log file 'logf'\n",
    "    logf.write( strn + '\\n')\n",
    "    print( strn )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lakes_shp_file = '../NSW_lakes/NSW_WaterBody.shp'\n",
    "   # shape file containing polygons for waterbodies of interest.\n",
    "lake_min_area = 5000\n",
    "   # waterbody screening: minimum area size in sq.m. for a waterbody to be considered.\n",
    "lake_screen_perennial = True\n",
    "   # waterbody screening flag: keep only perennial waterbodies?\n",
    "polygon_buffer_width = 1.5\n",
    "   # buffering of lake edges: distance (in Landsat pixels) from lake edges where pixels are masked out\n",
    "lake_min_valid_prc = 10.0\n",
    "   # minimum percentage of valid (non-NaN) extent required for lake to be processed / saved\n",
    "\n",
    "pix_size_geo = 0.00025   \n",
    "   # reprojection to geodetic: desired geodetic pixel size\n",
    "lake_buffer_width_geo = 0.002  \n",
    "   # buffer width to add around lake polygon extents for processing, in geodetic units\n",
    "\n",
    "cloud_buffer_width = 12.0\n",
    "   # buffer width (in Landsat pixels) to add around cloud and cloud shadow pixels in cloud mask\n",
    "\n",
    "save_dir_path = \"/g/data/jr4/vis_data_v3.0/\"\n",
    "   # base directory path where the WQ outputs are saved: will be created if necessary; can contain existing data.\n",
    "append_to_existing_nc = False  #True\n",
    "   # flag: append to dataset on file if exists; otherwise, overwrite it (i.e. create new file).\n",
    "backup_if_replacing_nc = True\n",
    "   # flag: if existing dataset is overwritten, create backup first.\n",
    "    \n",
    "start_date = '2014-01-01'   # '2006-01-01'\n",
    "   # start date of time window, format YYYY-MM-DD; will be determined from .nc file if exists and if appending\n",
    "end_date = '2015-12-31'  # '2014-01-01'   # None\n",
    "   # end date, same format: if set to None, will use current date as end date\n",
    "\n",
    "wofs_dir_path = '/g/data2/fk4/wofs/current/pyramids/WaterSummary/0/'   \n",
    "   # base path where geoTiff WOFS data is located\n",
    "wofs_prc_thr = 90.0\n",
    "   # percentage threshold for WOFS water mask\n",
    "\n",
    "SWIR2_prc_thr = 1.0\n",
    "   # percentage threshold for SWIR2 water mask\n",
    "low_water_prc_thr = 50.0\n",
    "   # percentage threshold below which the low-water flag is triggered (compared to polygon extents)\n",
    "\n",
    "WQ_type = 'Alg2:TwoBand_TSS_calib'   # 'Alg1:(red+green)/2'\n",
    "   # (string) type of water quality algorithm to use in the code below\n",
    "alert_thresholds = None   # e.g. (2000,2400)\n",
    "   # red/amber/green thresholds, defined as tuple (green_to_amber,amber_to_red); None if undefined\n",
    "    \n",
    "netcdf_time_units = \"seconds since 1970-01-01 00:00:00.0\"\n",
    "   # time units to use when saving data to NetCDF format\n",
    "netcdf_time_calendar = \"standard\"\n",
    "   # calendar to use when saving data to NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of satellites whose dataset will be used for WQ processing. These definitions need \n",
    "# to be determined on the basis of the current AGDC dataset (module) used.\n",
    "\n",
    "# Landsat 8 definitions:\n",
    "sat_1 = { 'API_sat_str' : 'ls8_nbar_albers',  # satellite string for API queries\n",
    "          'API_plat_str' : 'LANDSAT_8',       # satellite platform string for API queries\n",
    "          'green_satur_bit' : 2,              # PQ saturation bit for green band for this satellite\n",
    "          'red_satur_bit' : 3,                # PQ saturation bit for red band for this satellite\n",
    "          'swir2_satur_bit' : 7,              # PQ saturation bit for SWIR2 band for this satellite\n",
    "          'Alg2_calib_fac' : 3957.0,          # calibration constant (multiplicative factor) for selected WQ algorithm\n",
    "          'Alg2_calib_exp' : 1.6436 }         # calibration constant (exponent) for selected WQ algorithm\n",
    "\n",
    "# Landsat 7 definitions:\n",
    "sat_2 = { 'API_sat_str' : 'ls7_nbar_albers',\n",
    "          'API_plat_str' : 'LANDSAT_7',\n",
    "          'green_satur_bit' : 1,\n",
    "          'red_satur_bit' : 2,\n",
    "          'swir2_satur_bit' : 7,\n",
    "          'Alg2_calib_fac' : 3983.0,\n",
    "          'Alg2_calib_exp' : 1.6246 }\n",
    "\n",
    "# Landsat 5 definitions:\n",
    "sat_3 = { 'API_sat_str' : 'ls5_nbar_albers',   # can define LS5 here, even though we don't have any LS5 data\n",
    "          'API_plat_str' : 'LANDSAT_5',\n",
    "          'green_satur_bit' : 1,\n",
    "          'red_satur_bit' : 2,\n",
    "          'swir2_satur_bit' : 7,\n",
    "          'Alg2_calib_fac' : 3983.0,\n",
    "          'Alg2_calib_exp' : 1.6246 }\n",
    "\n",
    "WQ_sats = { 'sat1' : sat_1,   # resulting dictionary of all satellite defs to use\n",
    "            'sat2' : sat_2,\n",
    "            'sat3' : sat_3 }\n",
    "\n",
    "API_defs = { 'nbar_product_str' : 'nbar',      # NBAR product string\n",
    "             'red_band_str' : 'red',           # NBAR red band string\n",
    "             'green_band_str' : 'green',       # NBAR green band string\n",
    "             'swir2_band_str' : 'swir2',       # NBAR swir2 band string\n",
    "             'pq_product_str' : 'pqa',         # PQ product string\n",
    "             'pq_band_str' : 'pixelquality',   # PQ band string\n",
    "             'contig_bit' : 8,      # PQ contiguity bit \n",
    "             'land_bit' : 9,        # PQ land/sea bit\n",
    "             'cloud_bit1' : 10,     # PQ cloud/shadow bits\n",
    "             'cloud_bit2' : 11,\n",
    "             'cloud_bit3' : 12,\n",
    "             'cloud_bit4' : 13 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Projection to use for final maps: here using geodetic, could be user-specific (would require some re-coding)\n",
    "geo_proj = osr.SpatialReference ()\n",
    "geo_proj.ImportFromEPSG ( 4326 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if alert_thresholds==None:\n",
    "    alert_thresholds = np.nan\n",
    "else:\n",
    "    alert_thresholds = np.array(alert_thresholds)\n",
    "    if len(alert_thresholds)!=2 or alert_thresholds[0]>=alert_thresholds[1]:\n",
    "        raise RuntimeError(\"Alert thresholds not defined properly (2 values in increasing order).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not wofs_dir_path.endswith(\"/\"):\n",
    "    wofs_dir_path = wofs_dir_path + \"/\"\n",
    "\n",
    "# log file:\n",
    "log_base_path = save_dir_path + \"WQproc_\" + datetime.today().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Create directory if necessary:\n",
    "if not save_dir_path.endswith(\"/\"):\n",
    "    save_dir_path = save_dir_path + \"/\"\n",
    "\n",
    "if not os.path.exists( save_dir_path ):\n",
    "    os.makedirs( save_dir_path )\n",
    "    if not os.path.exists( save_dir_path ):\n",
    "        raise RuntimeError(\"Could not create directory '{}'.\".format(save_dir_path) )\n",
    "    log_file = open(log_base_path + \".log\", \"w\")\n",
    "    write_to_log( log_file, \"Created results directory '{}'.\".format(save_dir_path) )\n",
    "else:\n",
    "    log_file = open(log_base_path + \".log\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AGDC defs:\n",
    "dc_api = datacube.api.API()\n",
    "dc_dc = datacube.Datacube()\n",
    "# ae = AnalyticsEngine()\n",
    "# ee = ExecutionEngine()\n",
    "\n",
    "dc_prods = dc_dc.list_products()\n",
    "\n",
    "pqbnd = API_defs['pq_band_str']\n",
    "nbar_prod = API_defs['nbar_product_str']\n",
    "pq_prod = API_defs['pq_product_str']\n",
    "rbnd = API_defs['red_band_str']\n",
    "gbnd = API_defs['green_band_str']\n",
    "s2bnd = API_defs['swir2_band_str']\n",
    "\n",
    "contig_bit = API_defs['contig_bit']\n",
    "land_bit = API_defs['land_bit']\n",
    "cloud_bit1 = API_defs['cloud_bit1']\n",
    "cloud_bit2 = API_defs['cloud_bit2']\n",
    "cloud_bit3 = API_defs['cloud_bit3']\n",
    "cloud_bit4 = API_defs['cloud_bit4']\n",
    "\n",
    "cloud_bit_mask = (1<<cloud_bit1 | 1<<cloud_bit2 | 1<<cloud_bit3 | 1<<cloud_bit4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading waterbody polygons from `.shp` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open .shp file:\n",
    "lakes_vec = ogr.Open( lakes_shp_file )\n",
    "if lakes_vec==None:\n",
    "    raise RuntimeError( \"Cannot open shape file '{}'.\".format(lakes_shp_file) )\n",
    "if lakes_vec.GetLayerCount()!=1:\n",
    "    raise RuntimeError( \"File '{}' has 0 or multiple layers (1 expected).\".format(lakes_shp_file) )\n",
    "lakes_lyr = lakes_vec.GetLayer(0)\n",
    "\n",
    "# List of waterbody names and attributes:\n",
    "n_ftr = lakes_lyr.GetFeatureCount()\n",
    "lname_list = list()\n",
    "peren_list = list()\n",
    "shape_list = list()\n",
    "for kk in range( n_ftr ):\n",
    "    ftr = lakes_lyr.GetFeature(kk)\n",
    "    lname_list.append(ftr.GetField('hydroname'))   # name\n",
    "    peren_list.append(ftr.GetField('perennia_1'))   # perennial flag\n",
    "    shape_list.append(ftr.GetField('shape_STAr'))   # area in sq.m.\n",
    "\n",
    "# Screen waterbodies of interest:\n",
    "tmp = ( np.array(shape_list)>=lake_min_area )\n",
    "if lake_screen_perennial: \n",
    "    tmp = ( (np.array(peren_list)=='Perennial') & tmp )\n",
    "lakes_ftr_idx = np.where(tmp)[0]\n",
    "n_lakes = len(lakes_ftr_idx)\n",
    "if n_lakes==0:\n",
    "    raise RuntimeError( \"No waterbody left to process after screening.\")\n",
    "lakes_name_list = [ lname_list[ii] for ii in lakes_ftr_idx ]\n",
    "shape_list = [ shape_list[ii] for ii in lakes_ftr_idx ]\n",
    "lakes_dispname_list = lakes_name_list[:]   # list copy\n",
    "\n",
    "# Check for duplicated names, rename them (append _0, _1, etc.):\n",
    "renamed_cntr = 0\n",
    "for kk in range(n_lakes):\n",
    "    tmp = [ i for i,s in enumerate(lakes_name_list) if s==lakes_name_list[kk] ]\n",
    "    n_tmp = len(tmp)\n",
    "    if n_tmp>1:\n",
    "        renamed_cntr = renamed_cntr + 1\n",
    "        for ii,tt in enumerate(tmp):\n",
    "            lakes_name_list[tt] = lakes_name_list[tt] + \"_\" + str(ii)\n",
    "\n",
    "lakes_name_list = [ lakes_name_list[ii].replace(\" \",\"_\") for ii in range(n_lakes) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape file '../NSW_lakes/NSW_WaterBody.shp' info:\n",
      "  There are 1947 waterbodies in the file '../NSW_lakes/NSW_WaterBody.shp'.\n",
      "  A total of 912 perennial lakeswith area of more than 5000 sq.m. will be processed.\n",
      "  A total of 64 lake names were re-labelled to avoid duplicates.\n"
     ]
    }
   ],
   "source": [
    "# Log info:\n",
    "write_to_log( log_file, \"Shape file '{}' info:\".format(lakes_shp_file) )\n",
    "write_to_log( log_file, \"  There are {} waterbodies in the file '{}'.\".format(n_ftr,lakes_shp_file) )\n",
    "tmp = \"  A total of {} {}\".format(n_lakes,\"perennial lakes\" if lake_screen_perennial else \"lakes\")\n",
    "tmp = tmp + \"with area of more than {} sq.m. will be processed.\".format(lake_min_area)\n",
    "write_to_log( log_file, tmp )\n",
    "\n",
    "if renamed_cntr>0:\n",
    "    write_to_log( log_file, \"  A total of {} lake names were re-labelled to avoid duplicates.\".format(renamed_cntr) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration over polygons ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing polygon nr. 13 of 912 ...\n",
      "  Polygon name: EIGHT MILE\n",
      "  Surface area is 329.94 ha.\n",
      "  Polygon extents: min lon.: 145.828515999 --> nearest pixel: 145.8285\n",
      "                   max lon.: 145.892210296 --> nearest pixel: 145.89225\n",
      "                   min lat.: -30.143219031 --> nearest pixel: -30.14325\n",
      "                   max lat.: -30.117280595 --> nearest pixel: -30.11725\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    85 time slices were found for the selected time window and ROI (2014-01-05 to 2015-11-24).\n",
      "    1 time slices were removed due to discrepancies with the PQ dataset.\n",
      "    84 time slices left after merging duplicated dates.\n",
      "    63 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    52 time slices were found for the selected time window and ROI (2014-01-13 to 2015-12-25).\n",
      "    52 time slices left after merging duplicated dates.\n",
      "    46 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There is no data available for this polygon/ROI.\n",
      "  Polygon area is 255 by 144 pixels.\n",
      "\n",
      "Processing polygon nr. 16 of 912 ...\n",
      "  Polygon name: HUNGERFORD\n",
      "  Surface area is 278.92 ha.\n",
      "  Polygon extents: min lon.: 145.822791001 --> nearest pixel: 145.82275\n",
      "                   max lon.: 145.855705994 --> nearest pixel: 145.85575\n",
      "                   min lat.: -30.002 --> nearest pixel: -30.002\n",
      "                   max lat.: -29.976487686 --> nearest pixel: -29.97625\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    85 time slices were found for the selected time window and ROI (2014-01-05 to 2015-11-24).\n",
      "    1 time slices were removed due to discrepancies with the PQ dataset.\n",
      "    84 time slices left after merging duplicated dates.\n",
      "    61 time slices left following PQ & polygon masking.\n",
      "    2 time slices left following SWIR2 masking.\n",
      "    2 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    52 time slices were found for the selected time window and ROI (2014-01-13 to 2015-12-25).\n",
      "    52 time slices left after merging duplicated dates.\n",
      "    46 time slices left following PQ & polygon masking.\n",
      "    2 time slices left following SWIR2 masking.\n",
      "    2 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There are 4 time slices in the resulting multi-sensor time series.\n",
      "  Existing NetCDF file backed up to '/g/data/jr4/vis_data_v3.0/backup/HUNGERFORD.nc'.\n",
      "  WQ and ancillary data saved to file '/g/data/jr4/vis_data_v3.0/HUNGERFORD.nc'.\n",
      "  Total time processing polygon (h:m:s): 0:02:12.288473\n",
      "    Processing time (sec.) per ha per day (time window: 730 days): 0.000649717133154\n",
      "    Processing time (sec.) per ha per time slice (all 137 available): 0.00346199640294\n",
      "    Processing time (sec.) per ha per time slice (final series: 4 dates): 0.118573376801\n",
      "  Polygon area is 139 by 130 pixels.\n",
      "\n",
      "Processing polygon nr. 18 of 912 ...\n",
      "  Polygon name: ULAH\n",
      "  Surface area is 28.17 ha.\n",
      "  Polygon extents: min lon.: 147.836192519 --> nearest pixel: 147.836\n",
      "                   max lon.: 147.856779942 --> nearest pixel: 147.857\n",
      "                   min lat.: -30.055961339 --> nearest pixel: -30.056\n",
      "                   max lat.: -30.042915688 --> nearest pixel: -30.04275\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    38 time slices were found for the selected time window and ROI (2014-01-14 to 2015-11-17).\n",
      "    38 time slices left after merging duplicated dates.\n",
      "    28 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    15 time slices were found for the selected time window and ROI (2014-04-12 to 2015-12-27).\n",
      "    15 time slices left after merging duplicated dates.\n",
      "    11 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There is no data available for this polygon/ROI.\n",
      "  Polygon area is 87 by 70 pixels.\n",
      "\n",
      "Processing polygon nr. 22 of 912 ...\n",
      "  Polygon name: BROADMEADOWS\n",
      "  Surface area is 120.68 ha.\n",
      "  Polygon extents: min lon.: 144.99558678 --> nearest pixel: 144.9955\n",
      "                   max lon.: 145.013861129 --> nearest pixel: 145.014\n",
      "                   min lat.: -31.670505215 --> nearest pixel: -31.67075\n",
      "                   max lat.: -31.649072112 --> nearest pixel: -31.649\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    85 time slices were found for the selected time window and ROI (2014-01-05 to 2015-11-24).\n",
      "    1 time slices were removed due to discrepancies with the PQ dataset.\n",
      "    84 time slices left after merging duplicated dates.\n",
      "    56 time slices left following PQ & polygon masking.\n",
      "    19 time slices left following SWIR2 masking.\n",
      "    19 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    51 time slices were found for the selected time window and ROI (2014-01-13 to 2015-12-25).\n",
      "    51 time slices left after merging duplicated dates.\n",
      "    45 time slices left following PQ & polygon masking.\n",
      "    11 time slices left following SWIR2 masking.\n",
      "    11 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There are 30 time slices in the resulting multi-sensor time series.\n",
      "  Existing NetCDF file backed up to '/g/data/jr4/vis_data_v3.0/backup/BROADMEADOWS.nc'.\n",
      "  WQ and ancillary data saved to file '/g/data/jr4/vis_data_v3.0/BROADMEADOWS.nc'.\n",
      "  Total time processing polygon (h:m:s): 0:02:06.689771\n",
      "    Processing time (sec.) per ha per day (time window: 730 days): 0.00143809478888\n",
      "    Processing time (sec.) per ha per time slice (all 136 available): 0.00771918526384\n",
      "    Processing time (sec.) per ha per time slice (final series: 30 dates): 0.0349936398628\n",
      "  Polygon area is 80 by 105 pixels.\n",
      "\n",
      "Processing polygon nr. 23 of 912 ...\n",
      "  Polygon name: TILTAGARA\n",
      "  Surface area is 87.13 ha.\n",
      "  Polygon extents: min lon.: 144.862359702 --> nearest pixel: 144.86225\n",
      "                   max lon.: 144.877380225 --> nearest pixel: 144.8775\n",
      "                   min lat.: -31.863121913 --> nearest pixel: -31.86325\n",
      "                   max lat.: -31.845869395 --> nearest pixel: -31.84575\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    85 time slices were found for the selected time window and ROI (2014-01-05 to 2015-11-24).\n",
      "    1 time slices were removed due to discrepancies with the PQ dataset.\n",
      "    84 time slices left after merging duplicated dates.\n",
      "    27 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    51 time slices were found for the selected time window and ROI (2014-01-13 to 2015-12-25).\n",
      "    51 time slices left after merging duplicated dates.\n",
      "    13 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There is no data available for this polygon/ROI.\n",
      "  Polygon area is 65 by 85 pixels.\n",
      "\n",
      "Processing polygon nr. 24 of 912 ...\n",
      "  Polygon name: CORINYA\n",
      "  Surface area is 42.17 ha.\n",
      "  Polygon extents: min lon.: 144.418675323 --> nearest pixel: 144.4185\n",
      "                   max lon.: 144.430173109 --> nearest pixel: 144.43025\n",
      "                   min lat.: -32.232584791 --> nearest pixel: -32.23275\n",
      "                   max lat.: -32.22107571 --> nearest pixel: -32.221\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    84 time slices were found for the selected time window and ROI (2014-01-12 to 2015-11-15).\n",
      "    1 time slices were removed due to discrepancies with the PQ dataset.\n",
      "    42 time slices left after merging duplicated dates.\n",
      "    26 time slices left following PQ & polygon masking.\n",
      "    10 time slices left following SWIR2 masking.\n",
      "    10 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    25 time slices were found for the selected time window and ROI (2014-01-20 to 2015-12-25).\n",
      "    13 time slices left after merging duplicated dates.\n",
      "    12 time slices left following PQ & polygon masking.\n",
      "    3 time slices left following SWIR2 masking.\n",
      "    3 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There are 13 time slices in the resulting multi-sensor time series.\n",
      "  Existing NetCDF file backed up to '/g/data/jr4/vis_data_v3.0/backup/CORINYA.nc'.\n",
      "  WQ and ancillary data saved to file '/g/data/jr4/vis_data_v3.0/CORINYA.nc'.\n",
      "  Total time processing polygon (h:m:s): 0:01:29.085963\n",
      "    Processing time (sec.) per ha per day (time window: 730 days): 0.00289385957981\n",
      "    Processing time (sec.) per ha per time slice (all 109 available): 0.0193808944336\n",
      "    Processing time (sec.) per ha per time slice (final series: 13 dates): 0.162501345636\n",
      "  Polygon area is 50 by 58 pixels.\n",
      "\n",
      "Processing polygon nr. 27 of 912 ...\n",
      "  Polygon name: TRUNKETABELLA\n",
      "  Surface area is 47.61 ha.\n",
      "  Polygon extents: min lon.: 150.057125703 --> nearest pixel: 150.057\n",
      "                   max lon.: 150.080627746 --> nearest pixel: 150.08075\n",
      "                   min lat.: -36.059731535 --> nearest pixel: -36.05975\n",
      "                   max lat.: -36.043833367 --> nearest pixel: -36.04375\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    82 time slices were found for the selected time window and ROI (2014-01-09 to 2015-11-28).\n",
      "    82 time slices left after merging duplicated dates.\n",
      "    37 time slices left following PQ & polygon masking.\n",
      "    30 time slices left following SWIR2 masking.\n",
      "    30 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    69 time slices were found for the selected time window and ROI (2014-01-08 to 2015-12-29).\n",
      "    69 time slices left after merging duplicated dates.\n",
      "    46 time slices left following PQ & polygon masking.\n",
      "    44 time slices left following SWIR2 masking.\n",
      "    42 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There are 72 time slices in the resulting multi-sensor time series.\n",
      "  Existing NetCDF file backed up to '/g/data/jr4/vis_data_v3.0/backup/TRUNKETABELLA.nc'.\n",
      "  WQ and ancillary data saved to file '/g/data/jr4/vis_data_v3.0/TRUNKETABELLA.nc'.\n",
      "  Total time processing polygon (h:m:s): 0:02:15.465308\n",
      "    Processing time (sec.) per ha per day (time window: 730 days): 0.00389739570192\n",
      "    Processing time (sec.) per ha per time slice (all 151 available): 0.0188417143206\n",
      "    Processing time (sec.) per ha per time slice (final series: 72 dates): 0.0395152619778\n",
      "  Polygon area is 96 by 84 pixels.\n",
      "\n",
      "Processing polygon nr. 30 of 912 ...\n",
      "  Polygon name: COLLYTOOTELA\n",
      "  Surface area is 50.78 ha.\n",
      "  Polygon extents: min lon.: 148.95766539 --> nearest pixel: 148.9575\n",
      "                   max lon.: 148.991744822 --> nearest pixel: 148.99175\n",
      "                   min lat.: -29.762318715 --> nearest pixel: -29.7625\n",
      "                   max lat.: -29.732294417 --> nearest pixel: -29.73225\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    161 time slices were found for the selected time window and ROI (2014-01-07 to 2015-11-26).\n",
      "    81 time slices left after merging duplicated dates.\n",
      "    64 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    50 time slices were found for the selected time window and ROI (2014-01-31 to 2015-12-27).\n",
      "    27 time slices left after merging duplicated dates.\n",
      "    18 time slices left following PQ & polygon masking.\n",
      "    0 time slices left following SWIR2 masking.\n",
      "    --> No data left for this satellite.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There is no data available for this polygon/ROI.\n",
      "  Polygon area is 149 by 153 pixels.\n",
      "\n",
      "Processing polygon nr. 32 of 912 ...\n",
      "  Polygon name: TELEPHONE\n",
      "  Surface area is 15.79 ha.\n",
      "  Polygon extents: min lon.: 150.478864539 --> nearest pixel: 150.47875\n",
      "                   max lon.: 150.501114296 --> nearest pixel: 150.50125\n",
      "                   min lat.: -28.698919731 --> nearest pixel: -28.699\n",
      "                   max lat.: -28.691834392 --> nearest pixel: -28.69175\n",
      "  Date range for processing: 2014-01-01 (inclusive) to 2016-01-01 (exclusive)\n",
      "  Processing LANDSAT_8 data (satellite 1 of 3) ... \n",
      "    85 time slices were found for the selected time window and ROI (2014-01-07 to 2015-11-26).\n",
      "    85 time slices left after merging duplicated dates.\n",
      "    61 time slices left following PQ & polygon masking.\n",
      "    14 time slices left following SWIR2 masking.\n",
      "    11 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_7 data (satellite 2 of 3) ... \n",
      "    50 time slices were found for the selected time window and ROI (2014-01-08 to 2015-12-29).\n",
      "    50 time slices left after merging duplicated dates.\n",
      "    32 time slices left following PQ & polygon masking.\n",
      "    2 time slices left following SWIR2 masking.\n",
      "    2 time slices left following buffered polygon masking.\n",
      "  Processing LANDSAT_5 data (satellite 3 of 3) ... \n",
      "    No 'ls5_nbar_albers' satellite data available in current database, for the selected time window and ROI.\n",
      "  There are 13 time slices in the resulting multi-sensor time series.\n",
      "  Existing NetCDF file backed up to '/g/data/jr4/vis_data_v3.0/backup/TELEPHONE.nc'.\n",
      "  WQ and ancillary data saved to file '/g/data/jr4/vis_data_v3.0/TELEPHONE.nc'.\n",
      "  Total time processing polygon (h:m:s): 0:01:54.933749\n",
      "    Processing time (sec.) per ha per day (time window: 730 days): 0.00996807078749\n",
      "    Processing time (sec.) per ha per time slice (all 135 available): 0.0539014198139\n",
      "    Processing time (sec.) per ha per time slice (final series: 13 dates): 0.559745513452\n",
      "  Polygon area is 92 by 46 pixels.\n"
     ]
    }
   ],
   "source": [
    "cpu_data = np.ones((n_lakes,6))*np.nan\n",
    "\n",
    "for ftr in range(n_lakes):\n",
    "    poly_t0 = timer.time()\n",
    "    \n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "    if ftr==0:\n",
    "        ftrcnt = 10     # only testing a smaller nr of large waterbodies for now...\n",
    "    if shape_list[ftr]<100000:\n",
    "        continue\n",
    "    ftrcnt = ftrcnt - 1\n",
    "    if ftrcnt==0:\n",
    "        break\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "#######################################################################################################################\n",
    "    \n",
    "    \n",
    "    # Log info:\n",
    "    write_to_log( log_file, \"\\nProcessing polygon nr. {} of {} ...\".format(ftr+1,n_lakes) )\n",
    "    write_to_log( log_file, \"  Polygon name: {}\".format(lakes_dispname_list[ftr]) )\n",
    "    write_to_log( log_file, \"  Surface area is {} ha.\".format(round(shape_list[ftr]/10000.0,2)) )\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: ROI polygon coordinates and extents\n",
    "    ###########################################################################\n",
    "\n",
    "    # Read in polygon coordinates:\n",
    "    lake_ftr = lakes_lyr.GetFeature( lakes_ftr_idx[ftr] )\n",
    "    ring = lake_ftr.GetGeometryRef().GetGeometryRef(0)\n",
    "    poly_array_geo = np.array(ring.GetPoints())[:,0:2]    # lon / lat coords of polygon\n",
    "\n",
    "    tmp1 = min(poly_array_geo[:,0]) - lake_buffer_width_geo\n",
    "    min_lon = (tmp1 // pix_size_geo) * pix_size_geo\n",
    "\n",
    "    tmp2 = max(poly_array_geo[:,0]) + lake_buffer_width_geo\n",
    "    max_lon = (tmp2 // pix_size_geo) * pix_size_geo + pix_size_geo\n",
    "\n",
    "    tmp3 = min(poly_array_geo[:,1]) - lake_buffer_width_geo\n",
    "    min_lat = (tmp3 // pix_size_geo) * pix_size_geo\n",
    "\n",
    "    tmp4 = max(poly_array_geo[:,1]) + lake_buffer_width_geo\n",
    "    max_lat = (tmp4 // pix_size_geo) * pix_size_geo + pix_size_geo\n",
    "\n",
    "    # Coordinates of resulting WQ time series:\n",
    "    lon_vec = np.arange(min_lon, max_lon+pix_size_geo, pix_size_geo)\n",
    "    lat_vec = np.arange(min_lat, max_lat+pix_size_geo, pix_size_geo)\n",
    "    mean_lon = (max_lon+min_lon)/2.0\n",
    "\n",
    "    # Log info:\n",
    "    write_to_log( log_file, \"  Polygon extents: min lon.: {} --> nearest pixel: {}\".format(tmp1,min_lon) )\n",
    "    write_to_log( log_file, \"                   max lon.: {} --> nearest pixel: {}\".format(tmp2,max_lon) )\n",
    "    write_to_log( log_file, \"                   min lat.: {} --> nearest pixel: {}\".format(tmp3,min_lat) )\n",
    "    write_to_log( log_file, \"                   max lat.: {} --> nearest pixel: {}\".format(tmp4,max_lat) )\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    ### Iteration over polygons: check for existing dataset, determine ROI date range\n",
    "    ######################################################################################\n",
    "\n",
    "    # Check for existing dataset, determine date range:\n",
    "    roi_file_str = lakes_name_list[ftr] + \".nc\"\n",
    "    roi_save_name = save_dir_path + roi_file_str\n",
    "    roi_file_exists = os.path.isfile( roi_save_name )\n",
    "    tmp_org = append_to_existing_nc   # for log info only\n",
    "\n",
    "    roi_start_date = datetime( int(start_date[:4]), int(start_date[5:7]), int(start_date[8:10]) )\n",
    "\n",
    "    if append_to_existing_nc:\n",
    "        if roi_file_exists:\n",
    "            # read file attributes:\n",
    "            roi_nc_grp = Dataset(roi_save_name, mode='r')\n",
    "            roi_nc_dates = roi_nc_grp.variables['time']\n",
    "\n",
    "            # consistency checks between datasets:\n",
    "            if not (roi_nc_grp.variables['lat'][:]==lat_vec[::-1]).all():\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different latitudes.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not (roi_nc_grp.variables['lon'][:]==lon_vec).all():\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different longitudes.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not getattr(roi_nc_grp,'WQtype')==WQ_type:\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different WQ type.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not roi_nc_dates.calendar==netcdf_time_calendar:\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different time calendar.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not roi_nc_dates.units==netcdf_time_units:\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different time units.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not getattr(roi_nc_grp,'Name')==lakes_name_list[ftr]:\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different lake name.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not getattr(roi_nc_grp,'DisplayName')==lakes_dispname_list[ftr]:\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different display name.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not getattr(roi_nc_grp,'EPSG')==geo_proj.GetAttrValue(\"AUTHORITY\", 1):\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different EPSG.\".format(roi_save_name) )\n",
    "                continue\n",
    "            if not getattr(roi_nc_grp,'LakeType')==\"undefined\":   # clear/deep,turbid/shallow ... should be defined somehow!\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different lake type.\".format(roi_save_name) )\n",
    "                continue\n",
    "\n",
    "            tmp = getattr(roi_nc_grp,'GAR_thresholds')\n",
    "            c1 = np.size(tmp)!=np.size(alert_thresholds)\n",
    "            c2 = (np.size(tmp)==1 and (~np.isnan(tmp) | ~np.isnan(alert_thresholds)))\n",
    "            c3 = (np.size(tmp)==2 & ~(tmp==alert_thresholds).all())\n",
    "            if c1 or c2 or c3:\n",
    "                write_to_log(log_file,\"  Error appending to existing NetCDF '{}': different GAR thresholds.\".format(roi_save_name) )\n",
    "                continue\n",
    "\n",
    "            if len( roi_nc_dates )==0:\n",
    "                write_to_log(log_file,\"  Error: existing dataset '{}' has 0 dates.\".format(roi_save_name) )\n",
    "                continue\n",
    "\n",
    "            # get start date from existing .nc file:\n",
    "            roi_nc_last_date = num2date( roi_nc_dates[-1], roi_nc_dates.units, \n",
    "                                         roi_nc_dates.calendar )   # datetime.datetime object\n",
    "            roi_start_date = roi_nc_last_date + timedelta(days=1)\n",
    "\n",
    "            roi_nc_grp.close()\n",
    "        else:   # no existing file: cannot append; set start date from user input\n",
    "            append_to_existing_nc = False\n",
    "        #---end: if roi_file_exists\n",
    "    #---end: if append_to_existing_nc\n",
    "\n",
    "    if end_date==None:\n",
    "        roi_end_date = datetime.today()\n",
    "    else:\n",
    "        roi_end_date = datetime( int(end_date[:4]), int(end_date[5:7]), int(end_date[8:10]) )\n",
    "    roi_end_date = roi_end_date  + timedelta(days=1)\n",
    "\n",
    "    # Log info:\n",
    "    if roi_file_exists & tmp_org:\n",
    "        write_to_log( log_file, \"  Found existing NetCDF file '{}'; will append to it.\".format(roi_save_name) )\n",
    "        write_to_log( log_file, \"    Last date of existing time series: \" + roi_nc_last_date.strftime('%Y-%m-%d') )\n",
    "    elif ~roi_file_exists & tmp_org:\n",
    "        tmp = \"  Could not find existing NetCDF file '{}' to append to; will create new file.\".format(roi_save_name)\n",
    "        write_to_log( log_file, tmp )\n",
    "\n",
    "    tmp = \"  Date range for processing: \" + roi_start_date.strftime('%Y-%m-%d') + \" (inclusive) to \" \n",
    "    tmp = tmp + roi_end_date.strftime('%Y-%m-%d') + \" (exclusive)\"\n",
    "    write_to_log( log_file, tmp )\n",
    "\n",
    "    if roi_start_date >= roi_end_date:\n",
    "        write_to_log(log_file,\"  Error: start date of time window is after end date.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: load and process AGDC time series\n",
    "    ###########################################################################\n",
    "    \n",
    "    # AGDC defs:\n",
    "    dimensions = { 'lon' : { 'range': (min_lon, max_lon) },\n",
    "                   'lat' : { 'range': (max_lat, min_lat) },\n",
    "                   'time': { 'range': ( (roi_start_date.year, roi_start_date.month, roi_start_date.day), \n",
    "                                        (roi_end_date.year, roi_end_date.month, roi_end_date.day) ) } }\n",
    "\n",
    "    # Cycling through satellites:\n",
    "    n_sats = len(WQ_sats)\n",
    "    proc_init = True\n",
    "    wofs_init = True\n",
    "    wofs_error = False\n",
    "    concat_init = True\n",
    "    n_dates_orig = 0\n",
    "\n",
    "    for sat in range(n_sats):\n",
    "\n",
    "        ### Check for satellite availability:\n",
    "        sat_defs = WQ_sats.values()[sat]\n",
    "        sat_str = sat_defs['API_sat_str']\n",
    "        plat_str = sat_defs['API_plat_str']\n",
    "        write_to_log( log_file, \"  Processing {} data (satellite {} of {}) ... \".format(plat_str,sat+1,n_sats) )\n",
    "\n",
    "        if sat_str not in list(dc_prods['name']):\n",
    "            write_to_log( log_file, \"    Satellite data '{}' not available in current database.\".format(sat_str) )\n",
    "            continue\n",
    "        \n",
    "\n",
    "        ### Check for data availability:\n",
    "        query = { 'platform': plat_str,\n",
    "                  'product': nbar_prod,\n",
    "                  'dimensions': dimensions }\n",
    "        try:\n",
    "            tmp = dc_api.get_descriptor(query, include_storage_units=False)\n",
    "            if len( tmp[sat_str]['irregular_indices']['time'] )==0:\n",
    "                write_to_log( log_file, \"    No '{}' satellite data available in current database, for the selected time window and ROI.\".format(sat_str) )\n",
    "                continue\n",
    "        except:\n",
    "            write_to_log( log_file, \"    No '{}' satellite data available in current database, for the selected time window and ROI.\".format(sat_str) )\n",
    "            continue\n",
    "\n",
    "\n",
    "        ### Load data using AE/EE:\n",
    "        # ae.plan = list()   # appears to be \"resetting\" the AE definitions...\n",
    "        ae = AnalyticsEngine()\n",
    "        ee = ExecutionEngine()\n",
    "\n",
    "        ae_green = ae.create_array( (plat_str, nbar_prod), [gbnd], dimensions, 'ae_green' )\n",
    "        ae_red   = ae.create_array( (plat_str, nbar_prod), [rbnd], dimensions, 'ae_red' )\n",
    "        ae_swir2 = ae.create_array( (plat_str, nbar_prod), [s2bnd], dimensions, 'ae_swir2' )\n",
    "        aePQ     = ae.create_array( (plat_str, pq_prod), [pqbnd], dimensions, 'aePQ' )\n",
    "\n",
    "        # selected WQ algorithm:\n",
    "        if WQ_type=='Alg1:(red+green)/2':\n",
    "            aeWQ = ae.apply_expression( [ae_green, ae_red], '((array1 + array2) * 0.5)', 'aeWQ' )\n",
    "        elif WQ_type=='Alg2:TwoBand_TSS_calib':\n",
    "            cfac = sat_defs['Alg2_calib_fac']\n",
    "            cexp = sat_defs['Alg2_calib_exp']\n",
    "            # tmp = '( {} * ((array1 + array2)*0.00005)**{} )'.format(cfac,cexp)   # logical formulation, doesn't work!\n",
    "            tmp = '{}*exp(log((array1+array2)*0.00005)*{})'.format(cfac,cexp)   # hack to implement the power function...\n",
    "            aeWQ = ae.apply_expression( [ae_green, ae_red], tmp, 'aeWQ' )\n",
    "        # elif WQ_type='...':   ... add here further potential WQ algorithms.\n",
    "        else:\n",
    "            raise NotImplementedError( \"Water quality algorithm '{}' not implemented.\".format(WQ_type) )\n",
    "\n",
    "        ee.execute_plan(ae.plan)\n",
    "        WQ_xarray = ee.cache['aeWQ']['array_result']['aeWQ']\n",
    "        PQ_xarray = ee.cache['aePQ']['array_result'][pqbnd]\n",
    "        S2_xarray = ee.cache['ae_swir2']['array_result'][s2bnd]\n",
    "\n",
    "        # convert UTC dates to solar days:\n",
    "        for ii in range(WQ_xarray.shape[0]):\n",
    "            WQ_xarray['time'].values[ii] = solar_day(WQ_xarray['time'].values[ii], mean_lon).astype('datetime64[ns]')\n",
    "        for ii in range(PQ_xarray.shape[0]):\n",
    "            PQ_xarray['time'].values[ii] = solar_day(PQ_xarray['time'].values[ii], mean_lon).astype('datetime64[ns]')\n",
    "        for ii in range(S2_xarray.shape[0]):\n",
    "            S2_xarray['time'].values[ii] = solar_day(S2_xarray['time'].values[ii], mean_lon).astype('datetime64[ns]')\n",
    "        \n",
    "        n_dates = WQ_xarray.shape[0]\n",
    "        n_dates_orig = n_dates_orig + n_dates\n",
    "        tmp1 = WQ_xarray['time'].values\n",
    "        tmp = \"    {} time slices were found for the selected time window and ROI\".format(n_dates)\n",
    "        tmp = tmp + \" ({} to {}).\".format( str(tmp1[0])[:10], str(tmp1[-1])[:10] )\n",
    "        write_to_log( log_file, tmp )\n",
    "        \n",
    "        \n",
    "        ### Initialise proj. and poly. variables: once-only operations\n",
    "        if proc_init:\n",
    "            proc_init = False\n",
    "\n",
    "            # Satellite projection data: assume all satellites have same projection!\n",
    "            tmp = dc_prods[dc_prods.name==sat_str]['crs'].item().wkt\n",
    "            sat_proj = osr.SpatialReference( wkt=tmp )\n",
    "\n",
    "            sat_xvec = WQ_xarray['x'].values\n",
    "            sat_yvec = WQ_xarray['y'].values\n",
    "            npix_x = len(sat_xvec)\n",
    "            npix_y = len(sat_yvec)\n",
    "\n",
    "            sat_pix_size = sat_xvec[1] - sat_xvec[0]   # assume uniform square pixels\n",
    "            sat_geotx = ( sat_xvec[0], sat_pix_size, 0, sat_yvec[0], 0, -sat_pix_size )\n",
    "\n",
    "            # Polygon mask:\n",
    "            geo_to_sat_proj_tx = osr.CoordinateTransformation( geo_proj, sat_proj )\n",
    "            tmp = geo_to_sat_proj_tx.TransformPoints( poly_array_geo )\n",
    "            poly_array_xy = np.array( tmp )[:,:2]\n",
    "            polyPath = Path(poly_array_xy)\n",
    "            poly_mask_xy = np.ones(WQ_xarray.shape[1:], dtype='bool')\n",
    "\n",
    "            jj1 = np.where( sat_xvec<min(poly_array_xy[:,0]) )[0][-1]\n",
    "            jj2 = np.where( sat_xvec>max(poly_array_xy[:,0]) )[0][0]\n",
    "            ii1 = np.where( sat_yvec>max(poly_array_xy[:,1]) )[0][-1]\n",
    "            ii2 = np.where( sat_yvec<min(poly_array_xy[:,1]) )[0][0]\n",
    "            njj = jj2 - jj1 + 1\n",
    "\n",
    "            for ii in range( ii1, ii2+1 ):\n",
    "                tmp = np.stack( (sat_xvec[jj1:jj2+1], sat_yvec[ii]*np.ones(njj)), axis=1 )\n",
    "                poly_mask_xy[ii,jj1:jj2+1] = ~polyPath.contains_points( tmp )\n",
    "\n",
    "            n_pix_poly = np.sum( ~poly_mask_xy )\n",
    "            n_valid_pix_min = lake_min_valid_prc * n_pix_poly / 100.0\n",
    "\n",
    "            # Buffered polygon mask:\n",
    "            poly_mask_xy_bufd = poly_mask_xy.copy()\n",
    "            poly_mask_xy_bufd = expand_mask( poly_mask_xy_bufd, polygon_buffer_width )\n",
    "        #---end: if proc_init\n",
    "\n",
    "\n",
    "        ### Process time series: apply non-buffered-polygon + PQ masking to WQ and SWIR2 data\n",
    "        swir2_sat_bit = sat_defs['swir2_satur_bit']\n",
    "        red_sat_bit = sat_defs['red_satur_bit']\n",
    "        green_sat_bit = sat_defs['green_satur_bit']\n",
    "        other_bit_mask = ( 1<<green_sat_bit | 1<<red_sat_bit | 1<<swir2_sat_bit | 1<<contig_bit | 1<<land_bit )\n",
    "\n",
    "        keep_ind = np.zeros(n_dates).astype('bool')    # keep track of which dates (if any) to remove\n",
    "        PQ_dates = PQ_xarray['time'].values\n",
    "\n",
    "        for dd in range(n_dates):\n",
    "            cur_date = WQ_xarray['time'].values[dd]    # current date\n",
    "            idx = np.where(PQ_dates==cur_date)[0]\n",
    "            if len(idx)==0:    \n",
    "                continue   # time slice not found in PQ dataset: remove WQ slice (unable to mask properly)\n",
    "\n",
    "            # time slice found in PQ dataset: mask and keep date in time series\n",
    "            keep_ind[dd] = True\n",
    "            tmp = PQ_xarray[idx[0]].values.astype(int)\n",
    "            msk = ( (tmp & cloud_bit_mask)!=cloud_bit_mask ).astype(bool)\n",
    "            msk = expand_mask( msk, cloud_buffer_width )    # expanded cloud mask\n",
    "            msk = msk | ( (tmp & other_bit_mask)!=other_bit_mask ).astype(bool)   # saturation, sea, contiguity\n",
    "            msk = msk | poly_mask_xy   # non-buffered-polygon mask\n",
    "            WQ_xarray.values[dd][msk] = np.nan\n",
    "            S2_xarray.values[dd][msk] = np.nan\n",
    "        #---end: for dd in range(n_dates)\n",
    "\n",
    "        tmp = np.sum(keep_ind)\n",
    "        if n_dates!=tmp:\n",
    "            write_to_log( log_file, \"    {} time slices were removed due to discrepancies with the PQ dataset.\".format( n_dates-tmp) )\n",
    "        if tmp==0:\n",
    "            write_to_log( log_file, \"    --> No data left for this satellite.\" )\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[keep_ind]   # remove unwanted time slices\n",
    "        S2_xarray = S2_xarray[keep_ind]\n",
    "\n",
    "\n",
    "        ### Merge dates: this should theoretically process WQ and SWIR2 data identically...\n",
    "        WQ_xarray = merge_dup_dates( WQ_xarray.load() )\n",
    "        S2_xarray = merge_dup_dates( S2_xarray.load() )\n",
    "        write_to_log( log_file, \"    {} time slices left after merging duplicated dates.\".format(WQ_xarray.shape[0]) )\n",
    "\n",
    "\n",
    "        ### Remove \"empty\" time slices (ie. nr of valid pixels below threshold):\n",
    "        tmp = ( ~np.isnan(WQ_xarray) ).sum('x').sum('y').values\n",
    "        idx = np.where( tmp>=n_valid_pix_min )[0]\n",
    "        n_dates = len(idx)\n",
    "        write_to_log( log_file, \"    {} time slices left following PQ & polygon masking.\".format(n_dates) )\n",
    "        if n_dates==0:\n",
    "            write_to_log( log_file, \"    --> No data left for this satellite.\" )\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[idx]\n",
    "        S2_xarray = S2_xarray[idx]\n",
    "\n",
    "\n",
    "        ### Initialise WOFS variables: once-only (only if some data available / left)\n",
    "        if wofs_init:\n",
    "            wofs_init = False\n",
    "\n",
    "            # load WOFS data and reproject to UTM / satellite projection:\n",
    "            try:\n",
    "                wofs_array, wofs_proj, wofs_geotx, tmp1, tmp2 = load_wofs_data(\n",
    "                                                wofs_dir_path, min_lon, max_lon, min_lat, max_lat )\n",
    "            except RuntimeError as err:\n",
    "                write_to_log(log_file,\"  Error loading WOFS data: {}\".format(err) )\n",
    "                wofs_error = True\n",
    "                break\n",
    "\n",
    "            # WOFS gdal dataset:\n",
    "            nr, nc = wofs_array.shape\n",
    "            gdal_data = gdal.GetDriverByName( 'MEM' ).Create('', nc, nr, 1, gdal.GDT_Float32)\n",
    "            gdal_data.SetGeoTransform( wofs_geotx )\n",
    "            gdal_data.SetProjection( wofs_proj )\n",
    "            gdal_data.GetRasterBand(1).WriteArray( wofs_array, 0, 0)\n",
    "\n",
    "            # reprojected WOFS gdal dataset:\n",
    "            gdal_proj_data = gdal.GetDriverByName( 'MEM' ).Create('', len(sat_xvec), len(sat_yvec), 1, gdal.GDT_Float32)\n",
    "            gdal_proj_data.SetGeoTransform( sat_geotx )\n",
    "            gdal_proj_data.SetProjection( sat_proj.ExportToWkt() )\n",
    "            gdal_proj_data.GetRasterBand(1).WriteArray( np.zeros((len(sat_yvec),len(sat_xvec))), 0, 0) # prefill with 0\n",
    "            tmp = gdal.ReprojectImage( gdal_data, gdal_proj_data,\n",
    "                                       wofs_proj, sat_proj.ExportToWkt(),\n",
    "                                       gdal.GRA_Bilinear )    # gdal.GRA_NearestNeighbour by default\n",
    "\n",
    "            # WOFS mask, also masked by polygon (both non-buffered):\n",
    "            wofs_mask_xy = gdal_proj_data.ReadAsArray()\n",
    "            wofs_mask_xy = (wofs_mask_xy<wofs_prc_thr)\n",
    "            wofs_mask_xy = wofs_mask_xy | poly_mask_xy\n",
    "            n_pix_wofs = (~wofs_mask_xy).sum()\n",
    "        #---end: if wofs_init\n",
    "\n",
    "\n",
    "        ### Process time series: calculate WOFS and poly stats / flags, apply SWIR2 filter\n",
    "        low_water_flag_list = [\"undefined\" for ii in range(n_dates)]\n",
    "        wofs_flag_list = [\"ephemeral\" for ii in range(n_dates)]\n",
    "\n",
    "        for dd in range(n_dates):\n",
    "            WQ_arr = WQ_xarray[dd].values   # poly & PQ masked\n",
    "\n",
    "            # polygon stats:\n",
    "            tmp = np.sum( ~np.isnan(WQ_arr) )\n",
    "            n_pix_poly_nan = n_pix_poly - tmp   # low-water stats\n",
    "\n",
    "            # WOFS masking & stats (for stats only, not applied to WQ):\n",
    "            if n_pix_wofs!=0:\n",
    "                wq_tmp = WQ_arr.copy()\n",
    "                wq_tmp[wofs_mask_xy] = np.nan\n",
    "                tmp = np.sum( ~np.isnan(wq_tmp) )\n",
    "                n_pix_wofs_nan = n_pix_wofs - tmp   # WOFS stats\n",
    "\n",
    "            # apply SWIR2 mask, calculate water stats:\n",
    "            msk = S2_xarray[dd].values\n",
    "            msk = (msk>(SWIR2_prc_thr*100.0))   # SWIR2_prc_thr * 10000.0 / 100.0\n",
    "            WQ_xarray.values[dd][msk] = np.nan\n",
    "            n_water_pix_swir = np.sum( ~np.isnan(WQ_xarray[dd].values) )\n",
    "\n",
    "            # low-water flag:\n",
    "            nan_pix_prct = 100.0*n_pix_poly_nan/n_pix_poly\n",
    "            water_pix_prct = 100.0*n_water_pix_swir/n_pix_poly\n",
    "            if nan_pix_prct<=(100.0-low_water_prc_thr):\n",
    "                if water_pix_prct<(low_water_prc_thr-nan_pix_prct):\n",
    "                    low_water_flag = 'true'\n",
    "                elif water_pix_prct>=low_water_prc_thr:\n",
    "                    low_water_flag_list[dd] = 'false'\n",
    "            #   else: low_water_flag_list[dd] = 'undefined'\n",
    "            else:\n",
    "                if water_pix_prct<(low_water_prc_thr-nan_pix_prct):\n",
    "                    low_water_flag_list[dd] = 'true'\n",
    "            #   else: low_water_flag_list[dd] = 'undefined'\n",
    "\n",
    "            # WOFS water flag:\n",
    "            if n_pix_wofs!=0:\n",
    "                if n_water_pix_swir>(n_pix_wofs+n_pix_wofs_nan):\n",
    "                    wofs_flag_list[dd] = \"perennial: larger than WOFS {}% extents\".format(wofs_prc_thr)\n",
    "                elif n_water_pix_swir<(n_pix_wofs-n_pix_wofs_nan):\n",
    "                    wofs_flag_list[dd] = \"perennial: smaller than WOFS {}% extents\".format(wofs_prc_thr)\n",
    "                else:\n",
    "                    wofs_flag_list[dd] = \"perennial: undefined\"\n",
    "        #---end: for dd in range(n_dates)\n",
    "\n",
    "\n",
    "        ### Remove \"empty\" time slices (ie. nr of valid pixels below threshold):\n",
    "        tmp = ( ~np.isnan(WQ_xarray) ).sum('x').sum('y').values\n",
    "        idx = np.where( tmp>=n_valid_pix_min )[0]\n",
    "        n_dates = len(idx)\n",
    "        write_to_log( log_file, \"    {} time slices left following SWIR2 masking.\".format(n_dates) )\n",
    "        if n_dates==0:\n",
    "            write_to_log( log_file, \"    --> No data left for this satellite.\" )\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[idx]\n",
    "        low_water_flag_list = [low_water_flag_list[ii] for ii in idx]\n",
    "        wofs_flag_list = [wofs_flag_list[ii] for ii in idx]\n",
    "\n",
    "\n",
    "        ### Process time series: apply buffered-polygon to WQ\n",
    "        for dd in range(n_dates): \n",
    "            WQ_xarray.values[dd][poly_mask_xy_bufd] = np.nan\n",
    "\n",
    "\n",
    "        ### Remove \"empty\" time slices (ie. nr of valid pixels below threshold):\n",
    "        tmp = ( ~np.isnan(WQ_xarray) ).sum('x').sum('y').values\n",
    "        idx = np.where( tmp>=n_valid_pix_min )[0]\n",
    "        n_dates = len(idx)\n",
    "        write_to_log( log_file, \"    {} time slices left following buffered polygon masking.\".format(n_dates) )\n",
    "        if n_dates==0:\n",
    "            write_to_log( log_file, \"    --> No data left for this satellite.\" )\n",
    "            continue\n",
    "        WQ_xarray = WQ_xarray[idx]\n",
    "        low_water_flag_list = [low_water_flag_list[ii] for ii in idx]\n",
    "        wofs_flag_list = [wofs_flag_list[ii] for ii in idx]\n",
    "\n",
    "\n",
    "        ### Concatenate dataset with overall time series:\n",
    "        sat_flag_list = [plat_str for ii in range(n_dates)]\n",
    "        if concat_init:\n",
    "            # initialise variables\n",
    "            concat_init = False\n",
    "            WQ_xarray_TS = WQ_xarray.copy()\n",
    "            sat_flag_list_TS = sat_flag_list[:]   # list copy\n",
    "            low_water_flag_list_TS = low_water_flag_list[:]\n",
    "            wofs_flag_list_TS = wofs_flag_list[:]\n",
    "        else:\n",
    "            # concatenate dates\n",
    "            concat_dates = np.concatenate( (WQ_xarray_TS['time'].values, WQ_xarray['time'].values) )\n",
    "            sort_res = np.argsort(concat_dates)\n",
    "\n",
    "            # concatenate flags\n",
    "            tmp = sat_flag_list_TS + sat_flag_list   # list concatenation\n",
    "            sat_flag_list_TS = [tmp[ii] for ii in sort_res]\n",
    "            tmp = low_water_flag_list_TS + low_water_flag_list\n",
    "            low_water_flag_list_TS = [tmp[ii] for ii in sort_res]\n",
    "            tmp = wofs_flag_list_TS + wofs_flag_list\n",
    "            wofs_flag_list_TS = [tmp[ii] for ii in sort_res]\n",
    "\n",
    "            # concatenate data\n",
    "            sat1_idx = np.array( [ii for ii,ss in enumerate(sat_flag_list_TS) if ss!=plat_str] )\n",
    "            sat2_idx = np.array( [ii for ii,ss in enumerate(sat_flag_list_TS) if ss==plat_str] )\n",
    "            WQ_xarray_TS = xr.concat( [WQ_xarray_TS, WQ_xarray], dim='time', positions=[sat1_idx,sat2_idx] )\n",
    "        #---end: if concat_init\n",
    "\n",
    "    #---end: for sat in range(n_sats)\n",
    "    \n",
    "    if wofs_error:   # if error processing WOFS data, continue to next polygon\n",
    "        continue\n",
    "\n",
    "    if concat_init:   # no data available/left for this polygon / ROI\n",
    "        write_to_log(log_file,\"  There is no data available for this polygon/ROI.\")\n",
    "        write_to_log(log_file,\"  Polygon area is {} by {} pixels.\".format(npix_x,npix_y))\n",
    "        continue\n",
    "\n",
    "    n_dates = WQ_xarray_TS.shape[0]\n",
    "    write_to_log( log_file, \"  There are {} time slices in the resulting multi-sensor time series.\".format(n_dates) )\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: reprojecting to geodetic\n",
    "    ###########################################################################\n",
    "\n",
    "    # Reproject each time slice to geodetic:\n",
    "    npix_lon = len(lon_vec)\n",
    "    npix_lat = len(lat_vec)\n",
    "    n_dates, npix_y, npix_x = WQ_xarray_TS.shape\n",
    "    WQ_array_TS_geo = np.ones( (n_dates, npix_lat, npix_lon) ) * np.nan\n",
    "\n",
    "    for dd in range(n_dates):\n",
    "        # WQ gdal dataset:\n",
    "        gdal_data = gdal.GetDriverByName( 'MEM' ).Create('', npix_x, npix_y, 1, gdal.GDT_Float32)\n",
    "        gdal_data.SetGeoTransform( sat_geotx )\n",
    "        gdal_data.SetProjection( sat_proj.ExportToWkt() )\n",
    "        gdal_data.GetRasterBand(1).WriteArray( WQ_xarray_TS[dd].values, 0, 0)\n",
    "\n",
    "        # reprojected WQ gdal dataset:\n",
    "        gdal_proj_data = gdal.GetDriverByName( 'MEM' ).Create('', npix_lon, npix_lat, 1, gdal.GDT_Float32)\n",
    "        gdal_proj_data.SetGeoTransform( (lon_vec[0], pix_size_geo, 0, lat_vec[-1], 0, -pix_size_geo) )\n",
    "        gdal_proj_data.SetProjection( geo_proj.ExportToWkt() )\n",
    "\n",
    "        gdal_proj_data.GetRasterBand(1).WriteArray( np.ones((npix_lat,npix_lon))*np.nan, 0, 0)\n",
    "        tmp = gdal.ReprojectImage( gdal_data, gdal_proj_data,\n",
    "                                   sat_proj.ExportToWkt(), geo_proj.ExportToWkt(),\n",
    "                                   gdal.GRA_NearestNeighbour)\n",
    "                                   # gdal.GRA_Bilinear )   # found to eat away at edges of lake boundaries!...\n",
    "        WQ_array_TS_geo[dd] = gdal_proj_data.ReadAsArray()\n",
    "    #---end: for dd in range(n_dates)\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iterating over polygons: Lake stats & Green-Amber-Red flags\n",
    "    ###########################################################################\n",
    "\n",
    "    mean_vec = np.nanmean( WQ_array_TS_geo, (1,2) )\n",
    "    med_vec = np.nanmedian( WQ_array_TS_geo, (1,2) )\n",
    "    max_vec = np.nanmax( WQ_array_TS_geo, (1,2) )\n",
    "    min_vec = np.nanmin( WQ_array_TS_geo, (1,2) )\n",
    "\n",
    "    if np.size(alert_thresholds)==1:   # np.nan\n",
    "        GAR_flag_list = ['undefined' for ii in range(n_dates)]\n",
    "    else:\n",
    "        amb_idx = np.where( np.nansum( WQ_array_TS_geo>alert_thresholds[0], (1,2) )>0 )[0]\n",
    "        red_idx = np.where( np.nansum( WQ_array_TS_geo>alert_thresholds[1], (1,2) )>0 )[0]\n",
    "        GAR_flag_list = ['green' for ii in range(n_dates)]\n",
    "        for ii in amb_idx: GAR_flag_list[ii] = \"amber\"\n",
    "        for ii in red_idx: GAR_flag_list[ii] = \"red\"\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iterating over polygons: CPU info\n",
    "    ###########################################################################\n",
    "\n",
    "    poly_t1 = timer.time()\n",
    "    delta_t = poly_t1 - poly_t0\n",
    "\n",
    "    lake_area_ha = shape_list[ftr]/10000.0\n",
    "    box_area_ha = len(sat_xvec) * len(sat_yvec) * (sat_pix_size**2) / 10000.0\n",
    "    window_days = (roi_end_date - roi_start_date).days\n",
    "\n",
    "    # saving CPU data for later analysis:\n",
    "    cpu_data[ftr,0] = delta_t\n",
    "    cpu_data[ftr,1] = lake_area_ha\n",
    "    cpu_data[ftr,2] = box_area_ha\n",
    "    cpu_data[ftr,3] = window_days\n",
    "    cpu_data[ftr,4] = n_dates_orig\n",
    "    cpu_data[ftr,5] = n_dates\n",
    "\n",
    "\n",
    "    ###########################################################################\n",
    "    ### Iteration over polygons: saving to NetCDF\n",
    "    ###########################################################################\n",
    "    \n",
    "    # Extracting dates in datetime format:\n",
    "    dt_dates = np.zeros(n_dates).astype(datetime)\n",
    "    for ii in range(n_dates):\n",
    "        tmp = str( WQ_xarray_TS['time'].values[ii] )\n",
    "        dt_dates[ii] = datetime( int(tmp[0:4]), int(tmp[5:7]), int(tmp[8:10]), int(tmp[11:13]), \n",
    "                                 int(tmp[14:16]), int(tmp[17:19]), int(tmp[20:26]) )\n",
    "\n",
    "    dt_units = netcdf_time_units\n",
    "    dt_calendar = netcdf_time_calendar\n",
    "    dt_dates = date2num(dt_dates, units=dt_units, calendar=dt_calendar )\n",
    "\n",
    "    if append_to_existing_nc:   # append to existing NetCDF file\n",
    "\n",
    "        roi_nc_grp = Dataset(roi_save_name, mode='a')\n",
    "        n_ex_dates = roi_nc_grp.dimensions['time'].size   # nr of existing dates\n",
    "        idx = np.arange(n_ex_dates, n_ex_dates+n_dates)\n",
    "\n",
    "        # load and append to variables:\n",
    "        wq_vals = roi_nc_grp.variables['WQ_data']\n",
    "        wq_vals[idx,:,:] = WQ_array_TS_geo\n",
    "\n",
    "        times = roi_nc_grp.variables['time']\n",
    "        times[idx] = dt_dates\n",
    "\n",
    "        mean_vals = roi_nc_grp.variables['WQ_means']\n",
    "        mean_vals[idx] = mean_vec\n",
    "\n",
    "        med_vals = roi_nc_grp.variables['WQ_medians']\n",
    "        med_vals[idx] = med_vec\n",
    "\n",
    "        max_vals = roi_nc_grp.variables['WQ_maxvals']\n",
    "        max_vals[idx] = max_vec\n",
    "\n",
    "        min_vals = roi_nc_grp.variables['WQ_minvals']\n",
    "        min_vals[idx] = min_vec\n",
    "\n",
    "        sat_vals = roi_nc_grp.variables['satellite_flags']\n",
    "        sat_vals[idx] = np.array(sat_flag_list_TS)\n",
    "\n",
    "        water_vals = roi_nc_grp.variables['low_water_flags']\n",
    "        water_vals[idx] = np.array(low_water_flag_list_TS)\n",
    "\n",
    "        wofs_vals = roi_nc_grp.variables['wofs_flags']\n",
    "        wofs_vals[idx] = np.array(wofs_flag_list_TS)\n",
    "\n",
    "        gar_vals = roi_nc_grp.variables['GAR_flags']\n",
    "        gar_vals[idx] = np.array(GAR_flag_list)\n",
    "\n",
    "        roi_nc_grp.DateLastUpdated = datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
    "        roi_nc_grp.LastUpdateCPUinfo = (delta_t, lake_area_ha, box_area_ha, window_days, n_dates_orig, n_dates)\n",
    "\n",
    "        roi_nc_grp.close()\n",
    "        write_to_log( log_file, \"  WQ and ancillary data appended to file '{}'.\".format(roi_save_name) )\n",
    "\n",
    "    else:   # create new or replace existing NetCDF file\n",
    "\n",
    "        # backup the existing NetCDF file if desired\n",
    "        if roi_file_exists & backup_if_replacing_nc:\n",
    "            # create backup directory:\n",
    "            bak_dir_path = save_dir_path + 'backup/'\n",
    "            if not os.path.exists( bak_dir_path ):\n",
    "                os.makedirs( bak_dir_path )\n",
    "                if not os.path.exists( bak_dir_path ):\n",
    "                    write_to_log(log_file,\"  Error: could not create backup directory '{}'.\".format(bak_dir_path) )\n",
    "                    continue\n",
    "                write_to_log( log_file, \"  Created backup directory '{}'.\".format(bak_dir_path) )\n",
    "\n",
    "            # create backup:\n",
    "            tmp = bak_dir_path+roi_file_str\n",
    "            os.rename(roi_save_name, tmp)\n",
    "            if not os.path.isfile( tmp ):\n",
    "                write_to_log(log_file,\"  Error: could not back up NetCDF file '{}'.\".format(tmp) )\n",
    "                continue\n",
    "            write_to_log( log_file, \"  Existing NetCDF file backed up to '{}'.\".format(tmp) )\n",
    "        #---end if\n",
    "\n",
    "        # open NetCDF dataset:\n",
    "        roi_nc_grp = Dataset(roi_save_name, mode='w')   # overwrites existing file\n",
    "\n",
    "        # create NetCDF dimensions\n",
    "        time = roi_nc_grp.createDimension(\"time\", None)   # unlimited dimension\n",
    "        lat = roi_nc_grp.createDimension(\"lat\", len(lat_vec) )\n",
    "        lon = roi_nc_grp.createDimension(\"lon\", len(lon_vec) )\n",
    "\n",
    "        # create NetCDF variables\n",
    "        times = roi_nc_grp.createVariable(\"time\",\"f8\",(\"time\",))\n",
    "        lats = roi_nc_grp.createVariable(\"lat\",\"f8\",(\"lat\",))\n",
    "        lons = roi_nc_grp.createVariable(\"lon\",\"f8\",(\"lon\",))\n",
    "\n",
    "        wq_vals = roi_nc_grp.createVariable(\"WQ_data\",\"f8\",(\"time\",\"lat\",\"lon\",))\n",
    "        mean_vals = roi_nc_grp.createVariable(\"WQ_means\",\"f8\",(\"time\",))\n",
    "        med_vals = roi_nc_grp.createVariable(\"WQ_medians\",\"f8\",(\"time\",))\n",
    "        max_vals = roi_nc_grp.createVariable(\"WQ_maxvals\",\"f8\",(\"time\",))\n",
    "        min_vals = roi_nc_grp.createVariable(\"WQ_minvals\",\"f8\",(\"time\",))\n",
    "\n",
    "        sat_vals = roi_nc_grp.createVariable(\"satellite_flags\",\"S32\",(\"time\",))   # Define str length long enough for flags!\n",
    "        water_vals = roi_nc_grp.createVariable(\"low_water_flags\",\"S16\",(\"time\",))   # Define str length long enough for flags!\n",
    "        wofs_vals = roi_nc_grp.createVariable(\"wofs_flags\",\"S64\",(\"time\",))   # Define str length long enough for flags!\n",
    "        gar_vals = roi_nc_grp.createVariable(\"GAR_flags\",\"S8\",(\"time\",))   # Define str length long enough for flags!\n",
    "\n",
    "        # assign values to variables\n",
    "        times.units = dt_units\n",
    "        times.calendar = dt_calendar\n",
    "        times[:] = dt_dates\n",
    "        lons[:] = lon_vec\n",
    "        lats[:] = lat_vec[::-1]\n",
    "\n",
    "        wq_vals[:,:,:] = WQ_array_TS_geo\n",
    "        mean_vals[:] = mean_vec\n",
    "        med_vals[:] = med_vec\n",
    "        max_vals[:] = max_vec\n",
    "        min_vals[:] = min_vec\n",
    "\n",
    "        sat_vals[:] = np.array(sat_flag_list_TS)\n",
    "        water_vals[:] = np.array(low_water_flag_list_TS)\n",
    "        wofs_vals[:] = np.array(wofs_flag_list_TS)\n",
    "        gar_vals[:] = np.array(GAR_flag_list)\n",
    "\n",
    "        # dataset attributes:\n",
    "        roi_nc_grp.Name = lakes_name_list[ftr]\n",
    "        roi_nc_grp.DisplayName = lakes_dispname_list[ftr]\n",
    "        roi_nc_grp.EPSG = geo_proj.GetAttrValue(\"AUTHORITY\", 1)\n",
    "        roi_nc_grp.LakeType = \"undefined\"   # clear/deep vs. turbid/shallow ... should be defined somehow!\n",
    "        roi_nc_grp.WQtype = WQ_type\n",
    "        roi_nc_grp.GAR_thresholds = alert_thresholds\n",
    "        roi_nc_grp.DateCreated = datetime.now().strftime('%Y/%m/%d %H:%M:%S')\n",
    "        roi_nc_grp.DateLastUpdated = \"NA\"\n",
    "        roi_nc_grp.LastUpdateCPUinfo = (delta_t, lake_area_ha, box_area_ha, window_days, n_dates_orig, n_dates)\n",
    "\n",
    "        roi_nc_grp.close()\n",
    "\n",
    "        if not os.path.isfile( roi_save_name ):\n",
    "            write_to_log(log_file,\"  Error: could not create NetCDF file '{}'.\".format(roi_save_name) )\n",
    "            continue\n",
    "        write_to_log( log_file, \"  WQ and ancillary data saved to file '{}'.\".format(roi_save_name) )\n",
    "    #---end if append_to_existing_nc\n",
    "\n",
    "    # Log info:\n",
    "    tmp = delta_t / lake_area_ha\n",
    "    write_to_log( log_file, \"  Total time processing polygon (h:m:s): {}\".format(str(timedelta(seconds=delta_t))) )\n",
    "    write_to_log( log_file, \"    Processing time (sec.) per ha per day (time window: {} days): {}\".format(window_days,tmp/window_days) )\n",
    "    write_to_log( log_file, \"    Processing time (sec.) per ha per time slice (all {} available): {}\".format(n_dates_orig,tmp/n_dates_orig) )\n",
    "    write_to_log( log_file, \"    Processing time (sec.) per ha per time slice (final series: {} dates): {}\".format(n_dates,tmp/n_dates) )\n",
    "    \n",
    "    write_to_log(log_file,\"  Polygon area is {} by {} pixels.\".format(npix_x,npix_y))\n",
    "\n",
    "#---end: for ftr in range(n_lakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment / analysis of computing times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assessment plots of CPU times saved to file '/g/data/jr4/vis_data_v3.0/WQproc_2016-08-11_10-01-33.pdf'.\n",
      "CPU times saved to file '/g/data/jr4/vis_data_v3.0/WQproc_2016-08-11_10-01-33.pckl'.\n"
     ]
    }
   ],
   "source": [
    "rcp['axes.formatter.useoffset'] = False\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "idx = np.where( ~np.isnan(cpu_data[:,0]) )[0]\n",
    "n_cpu = len(idx)\n",
    "\n",
    "if n_cpu==0:\n",
    "    write_to_log( log_file, \"\\nNo CPU times to analyse.\" )\n",
    "else:\n",
    "    cpu_data = cpu_data[idx,:]\n",
    "    Y = cpu_data[:,0]\n",
    "    \n",
    "    with PdfPages(log_base_path + \".pdf\") as pdf:\n",
    "        plt.figure(figsize=(7,7))\n",
    "        plt.hist(Y, bins=60)\n",
    "        plt.xlabel(\"proc. time (sec.)\")\n",
    "        plt.title( 'Total proc. time ({} polygons with available data)'.format(n_cpu) )\n",
    "        pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,1]; plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"lake area (ha)\"); plt.title('Total lake area (polygon)')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('lake area (ha)'); plt.ylabel('proc. time (sec.)');\n",
    "        tmp = 'proc_time = {:.3g} * lake_area {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,2]; plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"box area (ha)\"); plt.title('Bounding box area (polygon extents)')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('box area (ha)'); plt.ylabel('proc. time (sec.)');\n",
    "        tmp = 'proc_time = {:.3g} * box_area {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,3]; plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"window length (days)\"); plt.title('Total nr. of days (time window)')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('window length (days)'); plt.ylabel('proc. time (sec.)');\n",
    "        tmp = 'proc_time = {:.3g} * window_days {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,4]; plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"nr. of dates\"); plt.title('Original nr. of time slices (all available)')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('nr. of dates'); plt.ylabel('proc. time (sec.)');\n",
    "        tmp = 'proc_time = {:.3g} * n_dates {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(14,7)); plt.subplot(1,2,1)\n",
    "        X = cpu_data[:,5]; plt.hist(X, bins=60)\n",
    "        plt.xlabel(\"nr. of dates\"); plt.title('Final nr. of time slices (final series)')\n",
    "        plt.subplot(1,2,2)\n",
    "        reg.fit( np.matrix(X).T, np.matrix(Y).T ); plt.scatter( X, Y, marker='.', edgecolor='blue', s=15 )\n",
    "        xx = np.matrix( (np.nanmin(X), np.nanmax(X)) ).T; plt.plot( xx, reg.predict( xx ), 'k--' )\n",
    "        plt.xlabel('nr. of dates'); plt.ylabel('proc. time (sec.)');\n",
    "        tmp = 'proc_time = {:.3g} * n_dates {:+.3g}'.format(reg.coef_[0][0],reg.intercept_[0])\n",
    "        tmp = tmp + '\\n(R^2 = {0:.3g})'.format(reg.score(np.matrix(X).T, np.matrix(Y).T))\n",
    "        plt.title(tmp); pdf.savefig(); plt.close()\n",
    "\n",
    "    write_to_log( log_file, \"\\nAssessment plots of CPU times saved to file '{}'.\".format(log_base_path + \".pdf\") )\n",
    "\n",
    "    tmp = log_base_path + \".pckl\"\n",
    "    with open(tmp, 'wb') as ff:\n",
    "        pickle.dump(cpu_data, ff)\n",
    "    write_to_log( log_file, \"CPU times saved to file '{}'.\".format(tmp) )\n",
    "#---end: if n_cpu==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+++ execution end +++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "write_to_log( log_file, \"\\n+++ execution end +++\\n\")\n",
    "log_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
